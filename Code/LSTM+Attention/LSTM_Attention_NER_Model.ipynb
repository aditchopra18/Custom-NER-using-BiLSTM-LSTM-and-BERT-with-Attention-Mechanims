{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Relevant Libraries\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import math\n",
    "\n",
    "# Importing the relevant files\n",
    "train_file = '../../Data/NCBItrainset_corpus.txt'\n",
    "dev_file = '../../Data/NCBIdevelopset_corpus.txt'\n",
    "model_name = '../../Models/LSTM_Attention_NER_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset file\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "def parse_dataset(lines):\n",
    "    paragraphs = []\n",
    "    paragraph = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            paragraph.append(line)\n",
    "        else:\n",
    "            if paragraph:\n",
    "                paragraphs.append(paragraph)\n",
    "                paragraph = []\n",
    "\n",
    "    if paragraph:\n",
    "        paragraphs.append(paragraph)\n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the different paragraphs and annotations\n",
    "def parse_paragraph(paragraph):\n",
    "    sentences = []\n",
    "    annotations = []\n",
    "    sentence = []\n",
    "\n",
    "    for line in paragraph:\n",
    "        if re.match(r'^\\d+\\|\\w\\|', line):\n",
    "            sentence.extend(line.split('|')[2].split())\n",
    "\n",
    "        elif re.match(r'^\\d+\\t\\d+\\t\\d+\\t', line):\n",
    "            start, end = int(line.split(\"\\t\")[1]), int(line.split(\"\\t\")[2])\n",
    "            annotations.append((start, end, line.split(\"\\t\")[3], line.split(\"\\t\")[4]))\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Labelling\n",
    "def tag_annotations(sentences, annotations):\n",
    "    tagged_sentences = []\n",
    "    char_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tags = ['O'] * len(sentence)    # Initialize all tags at \"O\"\n",
    "        word_starts = []\n",
    "        word_ends = []\n",
    "        char_pos = 0\n",
    "\n",
    "        for word in sentence:\n",
    "            word_starts.append(char_pos)\n",
    "            char_pos += len(word)\n",
    "            word_ends.append(char_pos)\n",
    "            char_pos += 1               # WhiteSpace Character\n",
    "\n",
    "        '''\n",
    "        Based on the character limits, the annotations are assigned\n",
    "        A custom IO tagging scheme is used\n",
    "        Labels are assigned on the basis of disease label in annotations\n",
    "        '''\n",
    "\n",
    "        for start, end, disease_info, label in annotations:\n",
    "            for i, (word_start, word_end) in enumerate(zip(word_starts, word_ends)):\n",
    "                if word_start >= start and word_end <= end:\n",
    "                    tags[i] = 'I-' + label\n",
    "                elif word_start < start < word_end or word_start < end < word_end:\n",
    "                    tags[i] = 'I-' + label\n",
    "\n",
    "        tagged_sentences.append((sentence, tags))\n",
    "\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Data/NCBItrainset_corpus.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Parsing the dataset file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m lines \u001b[38;5;241m=\u001b[39m read_dataset(train_file)\n\u001b[0;32m      3\u001b[0m paragraphs \u001b[38;5;241m=\u001b[39m parse_dataset(lines)\n\u001b[0;32m      5\u001b[0m all_sentences \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mread_dataset\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_dataset\u001b[39m(file_path):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m         lines \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lines\n",
      "File \u001b[1;32mc:\\Users\\Adit\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Data/NCBItrainset_corpus.txt'"
     ]
    }
   ],
   "source": [
    "# Parsing the dataset file\n",
    "lines = read_dataset(train_file)\n",
    "paragraphs = parse_dataset(lines)\n",
    "\n",
    "all_sentences = []\n",
    "all_tags = []\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    sentences, annotations = parse_paragraph(paragraph)\n",
    "    tagged_sentences = tag_annotations(sentences, annotations)\n",
    "    for sentence, tags in tagged_sentences:\n",
    "        all_sentences.append(sentence)\n",
    "        all_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the development dataset file\n",
    "dev_lines = read_dataset(dev_file)\n",
    "dev_paragraphs = parse_dataset(dev_lines)\n",
    "\n",
    "dev_sentences = []\n",
    "dev_tags = []\n",
    "\n",
    "for paragraph in dev_paragraphs:\n",
    "    sentences, annotations = parse_paragraph(paragraph)\n",
    "    tagged_sentences = tag_annotations(sentences, annotations)\n",
    "    for sentence, tags in tagged_sentences:\n",
    "        dev_sentences.append(sentence)\n",
    "        dev_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset class\n",
    "class LSTM_Attention_NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, word_encoder, tag_encoder, unknown_token='<UNK>'):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.word_encoder = word_encoder\n",
    "        self.tag_encoder = tag_encoder\n",
    "        self.unknown_token = unknown_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tags = self.tags[idx]\n",
    "\n",
    "        sentence_encoded = [self.word_encoder.get(word, self.word_encoder[self.unknown_token]) for word in sentence]\n",
    "        tags_encoded = self.tag_encoder.transform(tags)\n",
    "\n",
    "        return torch.tensor(sentence_encoded), torch.tensor(tags_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "all_words = [word for sentence in all_sentences for word in sentence]\n",
    "all_tags_flat = [tag for tags in all_tags for tag in tags]\n",
    "\n",
    "word_encoder = {word: idx for idx, word in enumerate(set(all_words))}\n",
    "unknown_token = '<UNK>' \n",
    "word_encoder[unknown_token] = len(word_encoder)  # Add unknown token\n",
    "# Done to prevent KeyError as some words might be out of vocabulary in testing dataset\n",
    "\n",
    "tag_encoder = LabelEncoder()\n",
    "tag_encoder.fit(all_tags_flat)\n",
    "\n",
    "dataset = LSTM_Attention_NERDataset(all_sentences, all_tags, word_encoder, tag_encoder, unknown_token)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Global Attention class\n",
    "class Attention (nn.Module):\n",
    "    def __init__ (self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        # Softmax converts it into a Probability distribution so that the weights are between 0 and 1\n",
    "        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)\n",
    "        weighted_output = lstm_output * attention_weights  # (batch_size, seq_len, hidden_dim)\n",
    "        return weighted_output # (batch_size, hidden_dim)\n",
    "\n",
    "class Attention_LSTM_NER_Model(nn.Module):\n",
    "    def __init__ (self, vocab_size, tagset_size, embedding_dim = 128, hidden_dim = 128):\n",
    "        super(Attention_LSTM_NER_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first = True)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "    def forward(self, i):\n",
    "        emb = self.embedding(i)\n",
    "        lstm_out , _ = self.lstm(emb)\n",
    "        att_out = self.attention(lstm_out)\n",
    "        tag_space = self.fc(att_out)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Defining the model characteristics\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = Attention_LSTM_NER_Model(len(word_encoder), len(tag_encoder.classes_)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5668080292249982\n",
      "Epoch 2, Loss: 1.460195293551997\n",
      "Epoch 3, Loss: 1.3680564447453147\n",
      "Epoch 4, Loss: 1.278433925227115\n",
      "Epoch 5, Loss: 1.1989712056360746\n",
      "Epoch 6, Loss: 1.1257870448263068\n",
      "Epoch 7, Loss: 1.0541532933712006\n",
      "Epoch 8, Loss: 0.9880018751872214\n",
      "Epoch 9, Loss: 0.9372260319559198\n",
      "Epoch 10, Loss: 0.8851738823087592\n",
      "Epoch 11, Loss: 0.8336068611395987\n",
      "Epoch 12, Loss: 0.7849007076338718\n",
      "Epoch 13, Loss: 0.7490074728664599\n",
      "Epoch 14, Loss: 0.7131066698777048\n",
      "Epoch 15, Loss: 0.6750933669115368\n",
      "Epoch 16, Loss: 0.6490483299682015\n",
      "Epoch 17, Loss: 0.6151810578609768\n",
      "Epoch 18, Loss: 0.5904948146719682\n",
      "Epoch 19, Loss: 0.5693604804967579\n",
      "Epoch 20, Loss: 0.5467846440641504\n",
      "Epoch 21, Loss: 0.5209900666224329\n",
      "Epoch 22, Loss: 0.5044770185884676\n",
      "Epoch 23, Loss: 0.4905813101090883\n",
      "Epoch 24, Loss: 0.4718010339297746\n",
      "Epoch 25, Loss: 0.46405067490903956\n",
      "Epoch 26, Loss: 0.4503212029996671\n",
      "Epoch 27, Loss: 0.44033464397254746\n",
      "Epoch 28, Loss: 0.42661342495366145\n",
      "Epoch 29, Loss: 0.41660073634825256\n",
      "Epoch 30, Loss: 0.4063230447079006\n",
      "Epoch 31, Loss: 0.3948278152628949\n",
      "Epoch 32, Loss: 0.3935422505203046\n",
      "Epoch 33, Loss: 0.3787434638330811\n",
      "Epoch 34, Loss: 0.3830325164292988\n",
      "Epoch 35, Loss: 0.36744942084739085\n",
      "Epoch 36, Loss: 0.36545094926106303\n",
      "Epoch 37, Loss: 0.3541607492064175\n",
      "Epoch 38, Loss: 0.3481470256259567\n",
      "Epoch 39, Loss: 0.3496444523334503\n",
      "Epoch 40, Loss: 0.3405581728408211\n",
      "Epoch 41, Loss: 0.3448147193381661\n",
      "Epoch 42, Loss: 0.33101269759629903\n",
      "Epoch 43, Loss: 0.331661476900703\n",
      "Epoch 44, Loss: 0.3248418397025058\n",
      "Epoch 45, Loss: 0.3282421207741687\n",
      "Epoch 46, Loss: 0.32256341372665603\n",
      "Epoch 47, Loss: 0.319675801615966\n",
      "Epoch 48, Loss: 0.3089959225372264\n",
      "Epoch 49, Loss: 0.30587961014948395\n",
      "Epoch 50, Loss: 0.30965561733434077\n",
      "Epoch 51, Loss: 0.3014744657434915\n",
      "Epoch 52, Loss: 0.3033786478795503\n",
      "Epoch 53, Loss: 0.2997857896905196\n",
      "Epoch 54, Loss: 0.2981384459294771\n",
      "Epoch 55, Loss: 0.29123333292572123\n",
      "Epoch 56, Loss: 0.2905501440951699\n",
      "Epoch 57, Loss: 0.29290142380877543\n",
      "Epoch 58, Loss: 0.28749923839380864\n",
      "Epoch 59, Loss: 0.2864182528696562\n",
      "Epoch 60, Loss: 0.28820412488360153\n",
      "Epoch 61, Loss: 0.28389113866969157\n",
      "Epoch 62, Loss: 0.2818705216050148\n",
      "Epoch 63, Loss: 0.2797438565053438\n",
      "Epoch 64, Loss: 0.27457470721320104\n",
      "Epoch 65, Loss: 0.2762858106901771\n",
      "Epoch 66, Loss: 0.2747001396982293\n",
      "Epoch 67, Loss: 0.27406094340901627\n",
      "Epoch 68, Loss: 0.271166135213877\n",
      "Epoch 69, Loss: 0.2691108200110887\n",
      "Epoch 70, Loss: 0.2687384972446843\n",
      "Epoch 71, Loss: 0.26436058275009455\n",
      "Epoch 72, Loss: 0.2677212694757863\n",
      "Epoch 73, Loss: 0.26233906259662226\n",
      "Epoch 74, Loss: 0.2620441925368811\n",
      "Epoch 75, Loss: 0.26048482523152705\n",
      "Epoch 76, Loss: 0.2634335622975701\n",
      "Epoch 77, Loss: 0.25673866546467733\n",
      "Epoch 78, Loss: 0.26002290021432073\n",
      "Epoch 79, Loss: 0.25744058348630605\n",
      "Epoch 80, Loss: 0.25831452051275655\n",
      "Epoch 81, Loss: 0.25525681164703873\n",
      "Epoch 82, Loss: 0.2496449574828148\n",
      "Epoch 83, Loss: 0.25019554048776627\n",
      "Epoch 84, Loss: 0.24962806897728065\n",
      "Epoch 85, Loss: 0.2502342784091046\n",
      "Epoch 86, Loss: 0.2501722938920322\n",
      "Epoch 87, Loss: 0.24749342077656797\n",
      "Epoch 88, Loss: 0.24285291527446948\n",
      "Epoch 89, Loss: 0.2448663362546971\n",
      "Epoch 90, Loss: 0.24301343175925708\n",
      "Epoch 91, Loss: 0.24382179387305913\n",
      "Epoch 92, Loss: 0.24000731855630875\n",
      "Epoch 93, Loss: 0.240485007041379\n",
      "Epoch 94, Loss: 0.23711109945648595\n",
      "Epoch 95, Loss: 0.23593931880436445\n",
      "Epoch 96, Loss: 0.2349479453344094\n",
      "Epoch 97, Loss: 0.2345397774326174\n",
      "Epoch 98, Loss: 0.2337825745344162\n",
      "Epoch 99, Loss: 0.2347475800075029\n",
      "Epoch 100, Loss: 0.23036001191327446\n",
      "Epoch 101, Loss: 0.2302010820100182\n",
      "Epoch 102, Loss: 0.22878344278586538\n",
      "Epoch 103, Loss: 0.2333203001241935\n",
      "Epoch 104, Loss: 0.22498956106995283\n",
      "Epoch 105, Loss: 0.22586717378152044\n",
      "Epoch 106, Loss: 0.22604498580882423\n",
      "Epoch 107, Loss: 0.2237495803519299\n",
      "Epoch 108, Loss: 0.2233371667956051\n",
      "Epoch 109, Loss: 0.22163112069431104\n",
      "Epoch 110, Loss: 0.2230495720317489\n",
      "Epoch 111, Loss: 0.21937717065999382\n",
      "Epoch 112, Loss: 0.22141756274198232\n",
      "Epoch 113, Loss: 0.21830657477441587\n",
      "Epoch 114, Loss: 0.22094407991359108\n",
      "Epoch 115, Loss: 0.2137686049467639\n",
      "Epoch 116, Loss: 0.21333275343242444\n",
      "Epoch 117, Loss: 0.2139882272795627\n",
      "Epoch 118, Loss: 0.2149413392732018\n",
      "Epoch 119, Loss: 0.21769162777223086\n",
      "Epoch 120, Loss: 0.20958240251792104\n",
      "Epoch 121, Loss: 0.2091199771354073\n",
      "Epoch 122, Loss: 0.2096786153943915\n",
      "Epoch 123, Loss: 0.21689800918102264\n",
      "Epoch 124, Loss: 0.21027514809056333\n",
      "Epoch 125, Loss: 0.21332393468994842\n",
      "Epoch 126, Loss: 0.20666075890001498\n",
      "Epoch 127, Loss: 0.2083331334747766\n",
      "Epoch 128, Loss: 0.20530316429702858\n",
      "Epoch 129, Loss: 0.2006538986767593\n",
      "Epoch 130, Loss: 0.20180093104902067\n",
      "Epoch 131, Loss: 0.20418312047657214\n",
      "Epoch 132, Loss: 0.1997974728675265\n",
      "Epoch 133, Loss: 0.2010967523643845\n",
      "Epoch 134, Loss: 0.1973122179900345\n",
      "Epoch 135, Loss: 0.1985330428732069\n",
      "Epoch 136, Loss: 0.197480182898672\n",
      "Epoch 137, Loss: 0.19453085468787895\n",
      "Epoch 138, Loss: 0.1953722211091142\n",
      "Epoch 139, Loss: 0.19253225693185078\n",
      "Epoch 140, Loss: 0.19523434419380992\n",
      "Epoch 141, Loss: 0.1929747423059062\n",
      "Epoch 142, Loss: 0.1904948541992589\n",
      "Epoch 143, Loss: 0.19171132343380073\n",
      "Epoch 144, Loss: 0.1882451709948088\n",
      "Epoch 145, Loss: 0.19392107348693044\n",
      "Epoch 146, Loss: 0.19301240106946543\n",
      "Epoch 147, Loss: 0.18561030012604438\n",
      "Epoch 148, Loss: 0.19212890102675087\n",
      "Epoch 149, Loss: 0.1886550595885829\n",
      "Epoch 150, Loss: 0.18537063171204768\n",
      "Epoch 151, Loss: 0.18743275576516202\n",
      "Epoch 152, Loss: 0.18781775157702596\n",
      "Epoch 153, Loss: 0.18497630208730698\n",
      "Epoch 154, Loss: 0.1860760336643771\n",
      "Epoch 155, Loss: 0.1799070772371794\n",
      "Epoch 156, Loss: 0.18037018807310806\n",
      "Epoch 157, Loss: 0.18045929623277565\n",
      "Epoch 158, Loss: 0.18029152719598068\n",
      "Epoch 159, Loss: 0.17829151216306185\n",
      "Epoch 160, Loss: 0.17685578528203463\n",
      "Epoch 161, Loss: 0.1769282492367845\n",
      "Epoch 162, Loss: 0.17514200116458692\n",
      "Epoch 163, Loss: 0.17709314783937052\n",
      "Epoch 164, Loss: 0.1801026612520218\n",
      "Epoch 165, Loss: 0.1721755599505023\n",
      "Epoch 166, Loss: 0.17412229235235013\n",
      "Epoch 167, Loss: 0.17406918226104034\n",
      "Epoch 168, Loss: 0.17245031618758253\n",
      "Epoch 169, Loss: 0.1699611270113995\n",
      "Epoch 170, Loss: 0.17082013856423528\n",
      "Epoch 171, Loss: 0.17023998459703044\n",
      "Epoch 172, Loss: 0.1696824176531089\n",
      "Epoch 173, Loss: 0.1694287322461605\n",
      "Epoch 174, Loss: 0.16654883579988228\n",
      "Epoch 175, Loss: 0.17023106468351265\n",
      "Epoch 176, Loss: 0.16824024288277878\n",
      "Epoch 177, Loss: 0.1657147380082231\n",
      "Epoch 178, Loss: 0.16644273188553357\n",
      "Epoch 179, Loss: 0.1641169470783911\n",
      "Epoch 180, Loss: 0.16542863336048627\n",
      "Epoch 181, Loss: 0.16390646268662654\n",
      "Epoch 182, Loss: 0.1619280919824776\n",
      "Epoch 183, Loss: 0.163873787577215\n",
      "Epoch 184, Loss: 0.16434010196673243\n",
      "Epoch 185, Loss: 0.16308317098178363\n",
      "Epoch 186, Loss: 0.1614885565481688\n",
      "Epoch 187, Loss: 0.15950905335576912\n",
      "Epoch 188, Loss: 0.15860360978465332\n",
      "Epoch 189, Loss: 0.15942873178344025\n",
      "Epoch 190, Loss: 0.16362444272166804\n",
      "Epoch 191, Loss: 0.15856430424671425\n",
      "Epoch 192, Loss: 0.15813866708623736\n",
      "Epoch 193, Loss: 0.15587477484031728\n",
      "Epoch 194, Loss: 0.15666190201514646\n",
      "Epoch 195, Loss: 0.15455684252083302\n",
      "Epoch 196, Loss: 0.15600912508211637\n",
      "Epoch 197, Loss: 0.15368627639193283\n",
      "Epoch 198, Loss: 0.1632266438713199\n",
      "Epoch 199, Loss: 0.15479466515152077\n",
      "Epoch 200, Loss: 0.15622212777012273\n",
      "Epoch 201, Loss: 0.15278404637386925\n",
      "Epoch 202, Loss: 0.15474545132172735\n",
      "Epoch 203, Loss: 0.15322175327884524\n",
      "Epoch 204, Loss: 0.1551485116544523\n",
      "Epoch 205, Loss: 0.15290746484932147\n",
      "Epoch 206, Loss: 0.14906022364371702\n",
      "Epoch 207, Loss: 0.15273475097982506\n",
      "Epoch 208, Loss: 0.14802476195128342\n",
      "Epoch 209, Loss: 0.15525868887964048\n",
      "Epoch 210, Loss: 0.1463551025249456\n",
      "Epoch 211, Loss: 0.14741895759576246\n",
      "Epoch 212, Loss: 0.14495838276649775\n",
      "Epoch 213, Loss: 0.14429555127495214\n",
      "Epoch 214, Loss: 0.14875313561213643\n",
      "Epoch 215, Loss: 0.1493284374867615\n",
      "Epoch 216, Loss: 0.1447274386882782\n",
      "Epoch 217, Loss: 0.14503022557810732\n",
      "Epoch 218, Loss: 0.14432765915989876\n",
      "Epoch 219, Loss: 0.14184588115466268\n",
      "Epoch 220, Loss: 0.1427534179467904\n",
      "Epoch 221, Loss: 0.14447582846409396\n",
      "Epoch 222, Loss: 0.14775826272211576\n",
      "Epoch 223, Loss: 0.14375758249508708\n",
      "Epoch 224, Loss: 0.14504513301347433\n",
      "Epoch 225, Loss: 0.14209536306167903\n",
      "Epoch 226, Loss: 0.14091731666734345\n",
      "Epoch 227, Loss: 0.13984998629281395\n",
      "Epoch 228, Loss: 0.1402871706768086\n",
      "Epoch 229, Loss: 0.13944905074803451\n",
      "Epoch 230, Loss: 0.14285952127293536\n",
      "Epoch 231, Loss: 0.13944041807400553\n",
      "Epoch 232, Loss: 0.1363757774233818\n",
      "Epoch 233, Loss: 0.13644482313018097\n",
      "Epoch 234, Loss: 0.13645137826863088\n",
      "Epoch 235, Loss: 0.13449142559578545\n",
      "Epoch 236, Loss: 0.14088336045020505\n",
      "Epoch 237, Loss: 0.13576062533416247\n",
      "Epoch 238, Loss: 0.13579995773340525\n",
      "Epoch 239, Loss: 0.13609844897138446\n",
      "Epoch 240, Loss: 0.13429097027370804\n",
      "Epoch 241, Loss: 0.1331854747902406\n",
      "Epoch 242, Loss: 0.1394333580606862\n",
      "Epoch 243, Loss: 0.13235697532562832\n",
      "Epoch 244, Loss: 0.13425261566513463\n",
      "Epoch 245, Loss: 0.13783001468369835\n",
      "Epoch 246, Loss: 0.13663812177745918\n",
      "Epoch 247, Loss: 0.13349191452327527\n",
      "Epoch 248, Loss: 0.13109798954897806\n",
      "Epoch 249, Loss: 0.13296513729973844\n",
      "Epoch 250, Loss: 0.13239820066251254\n",
      "Epoch 251, Loss: 0.12994128200960786\n",
      "Epoch 252, Loss: 0.13046680704543465\n",
      "Epoch 253, Loss: 0.12980699029408002\n",
      "Epoch 254, Loss: 0.1284605063694088\n",
      "Epoch 255, Loss: 0.1295547475548167\n",
      "Epoch 256, Loss: 0.13158499096569262\n",
      "Epoch 257, Loss: 0.12958499612776855\n",
      "Epoch 258, Loss: 0.13212910020037702\n",
      "Epoch 259, Loss: 0.12805089099626793\n",
      "Epoch 260, Loss: 0.1280670293459767\n",
      "Epoch 261, Loss: 0.12759937620476672\n",
      "Epoch 262, Loss: 0.13054790308600978\n",
      "Epoch 263, Loss: 0.12780445833739482\n",
      "Epoch 264, Loss: 0.1285807117819786\n",
      "Epoch 265, Loss: 0.13171751110961563\n",
      "Epoch 266, Loss: 0.12774307868982615\n",
      "Epoch 267, Loss: 0.12863857103021523\n",
      "Epoch 268, Loss: 0.13169724062869423\n",
      "Epoch 269, Loss: 0.12598703056573868\n",
      "Epoch 270, Loss: 0.13244509618533284\n",
      "Epoch 271, Loss: 0.12510695288840093\n",
      "Epoch 272, Loss: 0.12955835638077637\n",
      "Epoch 273, Loss: 0.12665875667804166\n",
      "Epoch 274, Loss: 0.12376419837145429\n",
      "Epoch 275, Loss: 0.13468638807535172\n",
      "Epoch 276, Loss: 0.12723583512400327\n",
      "Epoch 277, Loss: 0.1268078988712085\n",
      "Epoch 278, Loss: 0.1235040438018347\n",
      "Epoch 279, Loss: 0.12423977628350258\n",
      "Epoch 280, Loss: 0.1229102611541748\n",
      "Epoch 281, Loss: 0.12061589573951144\n",
      "Epoch 282, Loss: 0.12224474274798443\n",
      "Epoch 283, Loss: 0.12633948714325302\n",
      "Epoch 284, Loss: 0.12385065324212376\n",
      "Epoch 285, Loss: 0.12047765962779522\n",
      "Epoch 286, Loss: 0.12129063218047745\n",
      "Epoch 287, Loss: 0.12629966496636993\n",
      "Epoch 288, Loss: 0.12167997913141\n",
      "Epoch 289, Loss: 0.12061214309773947\n",
      "Epoch 290, Loss: 0.12192418994872194\n",
      "Epoch 291, Loss: 0.11916830135803473\n",
      "Epoch 292, Loss: 0.11962054738480794\n",
      "Epoch 293, Loss: 0.12072597914620449\n",
      "Epoch 294, Loss: 0.12183977585089834\n",
      "Epoch 295, Loss: 0.11971157887264301\n",
      "Epoch 296, Loss: 0.12122534882081182\n",
      "Epoch 297, Loss: 0.12492320765005915\n",
      "Epoch 298, Loss: 0.1252550729795506\n",
      "Epoch 299, Loss: 0.12012437316834142\n",
      "Epoch 300, Loss: 0.12582069674604818\n",
      "Epoch 301, Loss: 0.13190990666809835\n",
      "Epoch 302, Loss: 0.12754820679363452\n",
      "Epoch 303, Loss: 0.12035871591222913\n",
      "Epoch 304, Loss: 0.1213824919572002\n",
      "Epoch 305, Loss: 0.11881346235934057\n",
      "Epoch 306, Loss: 0.11971529788876835\n",
      "Epoch 307, Loss: 0.11720240096512594\n",
      "Epoch 308, Loss: 0.11929734208081898\n",
      "Epoch 309, Loss: 0.11862790662991374\n",
      "Epoch 310, Loss: 0.11969675162905141\n",
      "Epoch 311, Loss: 0.11551152561840258\n",
      "Epoch 312, Loss: 0.11687036388014492\n",
      "Epoch 313, Loss: 0.11639209032842987\n",
      "Epoch 314, Loss: 0.11653212085366249\n",
      "Epoch 315, Loss: 0.11574649026519374\n",
      "Epoch 316, Loss: 0.12393874655428685\n",
      "Epoch 317, Loss: 0.11419430541756906\n",
      "Epoch 318, Loss: 0.11877579810587983\n",
      "Epoch 319, Loss: 0.11946772999669376\n",
      "Epoch 320, Loss: 0.11461700950013964\n",
      "Epoch 321, Loss: 0.11503387497443902\n",
      "Epoch 322, Loss: 0.11334191826417257\n",
      "Epoch 323, Loss: 0.11514852039123837\n",
      "Epoch 324, Loss: 0.11384675308669868\n",
      "Epoch 325, Loss: 0.11552073375174873\n",
      "Epoch 326, Loss: 0.11448721009257593\n",
      "Epoch 327, Loss: 0.11318550479451292\n",
      "Epoch 328, Loss: 0.11422489279587018\n",
      "Epoch 329, Loss: 0.11525466861693483\n",
      "Epoch 330, Loss: 0.11370396123904931\n",
      "Epoch 331, Loss: 0.1135360392692842\n",
      "Epoch 332, Loss: 0.1171858949880851\n",
      "Epoch 333, Loss: 0.11347075414500739\n",
      "Epoch 334, Loss: 0.11310941745576106\n",
      "Epoch 335, Loss: 0.11886342497248399\n",
      "Epoch 336, Loss: 0.12012451338140588\n",
      "Epoch 337, Loss: 0.11618304821221452\n",
      "Epoch 338, Loss: 0.11249815437354539\n",
      "Epoch 339, Loss: 0.11668292256562333\n",
      "Epoch 340, Loss: 0.11497196692385171\n",
      "Epoch 341, Loss: 0.11679205318030558\n",
      "Epoch 342, Loss: 0.11759705233730768\n",
      "Epoch 343, Loss: 0.11806258343552288\n",
      "Epoch 344, Loss: 0.11198257772546065\n",
      "Epoch 345, Loss: 0.11196992252218096\n",
      "Epoch 346, Loss: 0.11334214732050896\n",
      "Epoch 347, Loss: 0.11292032015166785\n",
      "Epoch 348, Loss: 0.11231264432794169\n",
      "Epoch 349, Loss: 0.11440371192599598\n",
      "Epoch 350, Loss: 0.11184680422669963\n",
      "Epoch 351, Loss: 0.10972969763373074\n",
      "Epoch 352, Loss: 0.11005058516993334\n",
      "Epoch 353, Loss: 0.11078665483939021\n",
      "Epoch 354, Loss: 0.11021067947149277\n",
      "Epoch 355, Loss: 0.11014660702724206\n",
      "Epoch 356, Loss: 0.11449428294834338\n",
      "Epoch 357, Loss: 0.10883331700767342\n",
      "Epoch 358, Loss: 0.11216249650246218\n",
      "Epoch 359, Loss: 0.11037856732544146\n",
      "Epoch 360, Loss: 0.11072836365354688\n",
      "Epoch 361, Loss: 0.11499015085007015\n",
      "Epoch 362, Loss: 0.11255847113697152\n",
      "Epoch 363, Loss: 0.11500613567860503\n",
      "Epoch 364, Loss: 0.10936639889290459\n",
      "Epoch 365, Loss: 0.10868400031406629\n",
      "Epoch 366, Loss: 0.10958077758550644\n",
      "Epoch 367, Loss: 0.11023115994114625\n",
      "Epoch 368, Loss: 0.10875857778285679\n",
      "Epoch 369, Loss: 0.1107534697573436\n",
      "Epoch 370, Loss: 0.11104369653682959\n",
      "Epoch 371, Loss: 0.11125224867933675\n",
      "Epoch 372, Loss: 0.11035813822558052\n",
      "Epoch 373, Loss: 0.10921995675093249\n",
      "Epoch 374, Loss: 0.1107235660678462\n",
      "Epoch 375, Loss: 0.10735832669429089\n",
      "Epoch 376, Loss: 0.10745524526818802\n",
      "Epoch 377, Loss: 0.10944786334508344\n",
      "Epoch 378, Loss: 0.11068963475133244\n",
      "Epoch 379, Loss: 0.10872617305109375\n",
      "Epoch 380, Loss: 0.10965804040039841\n",
      "Epoch 381, Loss: 0.10888992828366004\n",
      "Epoch 382, Loss: 0.11345719781361129\n",
      "Epoch 383, Loss: 0.11920896035275962\n",
      "Epoch 384, Loss: 0.10929529859047186\n",
      "Epoch 385, Loss: 0.10820089957039607\n",
      "Epoch 386, Loss: 0.11364736447208806\n",
      "Epoch 387, Loss: 0.11009639717246357\n",
      "Epoch 388, Loss: 0.10690326929876678\n",
      "Epoch 389, Loss: 0.10987513473159388\n",
      "Epoch 390, Loss: 0.10707395239488075\n",
      "Epoch 391, Loss: 0.10976770833918922\n",
      "Epoch 392, Loss: 0.10816402988214242\n",
      "Epoch 393, Loss: 0.10709030083135555\n",
      "Epoch 394, Loss: 0.10738549655989597\n",
      "Epoch 395, Loss: 0.10620331793631378\n",
      "Epoch 396, Loss: 0.11387323862627934\n",
      "Epoch 397, Loss: 0.11131978858458369\n",
      "Epoch 398, Loss: 0.10794978696656854\n",
      "Epoch 399, Loss: 0.1066819747027598\n",
      "Epoch 400, Loss: 0.10491943937775336\n",
      "Epoch 401, Loss: 0.106960413487334\n",
      "Epoch 402, Loss: 0.10513706402362961\n",
      "Epoch 403, Loss: 0.11114405919062464\n",
      "Epoch 404, Loss: 0.11197603493928909\n",
      "Epoch 405, Loss: 0.10601663334589255\n",
      "Epoch 406, Loss: 0.10642292232889879\n",
      "Epoch 407, Loss: 0.11033358738610619\n",
      "Epoch 408, Loss: 0.10680591491492171\n",
      "Epoch 409, Loss: 0.10661506280303001\n",
      "Epoch 410, Loss: 0.10667833803515685\n",
      "Epoch 411, Loss: 0.10647454720578696\n",
      "Epoch 412, Loss: 0.10632102387516122\n",
      "Epoch 413, Loss: 0.10518056330712218\n",
      "Epoch 414, Loss: 0.10509532984150083\n",
      "Epoch 415, Loss: 0.10744415694161465\n",
      "Epoch 416, Loss: 0.10467127583136684\n",
      "Epoch 417, Loss: 0.10872520231886913\n",
      "Epoch 418, Loss: 0.10595418316753287\n",
      "Epoch 419, Loss: 0.10766079786576722\n",
      "Epoch 420, Loss: 0.10449896262664544\n",
      "Epoch 421, Loss: 0.10605645160141744\n",
      "Epoch 422, Loss: 0.10604051931908257\n",
      "Epoch 423, Loss: 0.1067893477646928\n",
      "Epoch 424, Loss: 0.10613219557624114\n",
      "Epoch 425, Loss: 0.1081723825711953\n",
      "Epoch 426, Loss: 0.10461681399886545\n",
      "Epoch 427, Loss: 0.11010387520256795\n",
      "Epoch 428, Loss: 0.10611137242889718\n",
      "Epoch 429, Loss: 0.10689746539451574\n",
      "Epoch 430, Loss: 0.10534283219787635\n",
      "Epoch 431, Loss: 0.10826543365654193\n",
      "Epoch 432, Loss: 0.10521726094578442\n",
      "Epoch 433, Loss: 0.10675768711064991\n",
      "Epoch 434, Loss: 0.10533738469606951\n",
      "Epoch 435, Loss: 0.10545420960376137\n",
      "Epoch 436, Loss: 0.10702042869831387\n",
      "Epoch 437, Loss: 0.10402295924723148\n",
      "Epoch 438, Loss: 0.10361562492816072\n",
      "Epoch 439, Loss: 0.10544483951832119\n",
      "Epoch 440, Loss: 0.10327949541571893\n",
      "Epoch 441, Loss: 0.10871797956918415\n",
      "Epoch 442, Loss: 0.10561988816449516\n",
      "Epoch 443, Loss: 0.10384657841763999\n",
      "Epoch 444, Loss: 0.10374994164234713\n",
      "Epoch 445, Loss: 0.10491308001311202\n",
      "Epoch 446, Loss: 0.10519283431533136\n",
      "Epoch 447, Loss: 0.10442359961177174\n",
      "Epoch 448, Loss: 0.103689467240321\n",
      "Epoch 449, Loss: 0.10458022493280862\n",
      "Epoch 450, Loss: 0.10269758169000086\n",
      "Epoch 451, Loss: 0.1043163126236514\n",
      "Epoch 452, Loss: 0.10329959051389444\n",
      "Epoch 453, Loss: 0.10648091019768464\n",
      "Epoch 454, Loss: 0.10565929114818573\n",
      "Epoch 455, Loss: 0.1038875462193238\n",
      "Epoch 456, Loss: 0.1031849078441921\n",
      "Epoch 457, Loss: 0.10260621860231224\n",
      "Epoch 458, Loss: 0.10250176077610568\n",
      "Epoch 459, Loss: 0.10364591938100363\n",
      "Epoch 460, Loss: 0.10313083015774425\n",
      "Epoch 461, Loss: 0.10371192348630805\n",
      "Epoch 462, Loss: 0.10886037114419435\n",
      "Epoch 463, Loss: 0.10532624372526218\n",
      "Epoch 464, Loss: 0.10261490568518639\n",
      "Epoch 465, Loss: 0.10334916118728488\n",
      "Epoch 466, Loss: 0.10154762712160223\n",
      "Epoch 467, Loss: 0.10654385093795626\n",
      "Epoch 468, Loss: 0.10676878769146769\n",
      "Epoch 469, Loss: 0.10181950522880805\n",
      "Epoch 470, Loss: 0.10795624534550466\n",
      "Epoch 471, Loss: 0.10536374543842517\n",
      "Epoch 472, Loss: 0.10143009401661784\n",
      "Epoch 473, Loss: 0.1031662964899289\n",
      "Epoch 474, Loss: 0.1033639156896817\n",
      "Epoch 475, Loss: 0.10222406950044005\n",
      "Epoch 476, Loss: 0.10262815046467279\n",
      "Epoch 477, Loss: 0.1025725206066119\n",
      "Epoch 478, Loss: 0.10191502627965651\n",
      "Epoch 479, Loss: 0.10470781063562945\n",
      "Epoch 480, Loss: 0.10200637922083076\n",
      "Epoch 481, Loss: 0.10529452660366108\n",
      "Epoch 482, Loss: 0.10416181444337494\n",
      "Epoch 483, Loss: 0.10508197114655846\n",
      "Epoch 484, Loss: 0.10231646071923406\n",
      "Epoch 485, Loss: 0.10421653013480336\n",
      "Epoch 486, Loss: 0.10200834401736134\n",
      "Epoch 487, Loss: 0.10438152069323942\n",
      "Epoch 488, Loss: 0.10598374452245862\n",
      "Epoch 489, Loss: 0.10540719722446643\n",
      "Epoch 490, Loss: 0.10640579226769899\n",
      "Epoch 491, Loss: 0.10554667424998786\n",
      "Epoch 492, Loss: 0.10321373002309549\n",
      "Epoch 493, Loss: 0.10611101847730185\n",
      "Epoch 494, Loss: 0.10283738531564411\n",
      "Epoch 495, Loss: 0.10228736777054637\n",
      "Epoch 496, Loss: 0.10172113276233799\n",
      "Epoch 497, Loss: 0.10272393561899662\n",
      "Epoch 498, Loss: 0.10200188857944388\n",
      "Epoch 499, Loss: 0.10106679689335196\n",
      "Epoch 500, Loss: 0.10188850682032735\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "for epoch in range(500):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        sentences, tags = zip(*batch)\n",
    "        sentences = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True).to(device)\n",
    "        tags = torch.nn.utils.rnn.pad_sequence(tags, batch_first=True, padding_value=-100).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sentences)\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        loss = criterion(outputs, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model as a .pth file\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "\n",
    "# Load the test dataset\n",
    "test_file = '../../Data/NCBItestset_corpus.txt'\n",
    "test_lines = read_dataset(test_file)\n",
    "test_paragraphs = parse_dataset(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing and storing the test dataset\n",
    "test_sentences = []\n",
    "test_tags = []\n",
    "\n",
    "for paragraph in test_paragraphs:\n",
    "    sentences, annotations = parse_paragraph(paragraph)\n",
    "    tagged_sentences = tag_annotations(sentences, annotations)\n",
    "    for sentence, tags in tagged_sentences:\n",
    "        test_sentences.append(sentence)\n",
    "        test_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention_LSTM_NER_Model(\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (embedding): Embedding(14805, 128)\n",
       "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
       "  (attention): Attention(\n",
       "    (attention): Linear(in_features=128, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the model file\n",
    "model = Attention_LSTM_NER_Model(len(word_encoder), len(tag_encoder.classes_)).to(device)\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "I-CompositeMention       0.00      0.00      0.00        89\n",
      "    I-DiseaseClass       0.00      0.00      0.00       255\n",
      "        I-Modifier       0.00      0.00      0.00       367\n",
      " I-SpecificDisease       0.40      0.73      0.52      1090\n",
      "                 O       0.97      0.96      0.97     18601\n",
      "\n",
      "          accuracy                           0.92     20402\n",
      "         macro avg       0.27      0.34      0.30     20402\n",
      "      weighted avg       0.91      0.92      0.91     20402\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adit\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Adit\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Adit\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Prepare the test data\n",
    "test_dataset = LSTM_Attention_NERDataset(test_sentences, test_tags, word_encoder, tag_encoder, '<UNK>')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "# Evaluate the model\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "result = \"../../Result/TestResults_AttentionLSTM_NER.txt\"\n",
    "with open(result, 'w') as t_file:\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            sentences, tags = zip(*batch)\n",
    "            sentences = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True).to(device)\n",
    "            tags = torch.nn.utils.rnn.pad_sequence(tags, batch_first=True, padding_value=-100).to(device)\n",
    "\n",
    "            outputs = model(sentences)\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            true_labels = tags.cpu().numpy()\n",
    "\n",
    "            mask = true_labels != -100\n",
    "            pred_labels = predictions[mask]\n",
    "            true_labels = true_labels[mask]\n",
    "\n",
    "            pred_labels_decoded = tag_encoder.inverse_transform(pred_labels)\n",
    "            true_labels_decoded = tag_encoder.inverse_transform(true_labels)\n",
    "\n",
    "            for true_label, pred_label in zip(true_labels_decoded, pred_labels_decoded):\n",
    "                t_file.write(f'True: {true_label}, Pred: {pred_label}\\n')\n",
    "                all_true_labels.append(true_label)\n",
    "                all_pred_labels.append(pred_label)\n",
    "\n",
    "# Printing classification report\n",
    "report = classification_report(all_true_labels, all_pred_labels)\n",
    "print (report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
