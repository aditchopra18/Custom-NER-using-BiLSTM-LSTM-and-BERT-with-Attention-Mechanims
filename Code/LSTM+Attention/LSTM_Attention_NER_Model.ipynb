{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Relevant Libraries\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Importing the relevant files\n",
    "train_file = '../../Data/NCBItrainset_corpus.txt'\n",
    "model_name = '../../Models/LSTM_Attention_NER_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset file\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "def parse_dataset(lines):\n",
    "    paragraphs = []\n",
    "    paragraph = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            paragraph.append(line)\n",
    "        else:\n",
    "            if paragraph:\n",
    "                paragraphs.append(paragraph)\n",
    "                paragraph = []\n",
    "\n",
    "    if paragraph:\n",
    "        paragraphs.append(paragraph)\n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the different paragraphs and annotations\n",
    "def parse_paragraph(paragraph):\n",
    "    sentences = []\n",
    "    annotations = []\n",
    "    sentence = []\n",
    "\n",
    "    for line in paragraph:\n",
    "        if re.match(r'^\\d+\\|\\w\\|', line):\n",
    "            sentence.extend(line.split('|')[2].split())\n",
    "\n",
    "        elif re.match(r'^\\d+\\t\\d+\\t\\d+\\t', line):\n",
    "            start, end = int(line.split(\"\\t\")[1]), int(line.split(\"\\t\")[2])\n",
    "            annotations.append((start, end, line.split(\"\\t\")[3], line.split(\"\\t\")[4]))\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Labelling\n",
    "def tag_annotations(sentences, annotations):\n",
    "    tagged_sentences = []\n",
    "    char_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tags = ['O'] * len(sentence)    # Initialize all tags at \"O\"\n",
    "        word_starts = []\n",
    "        word_ends = []\n",
    "        char_pos = 0\n",
    "\n",
    "        for word in sentence:\n",
    "            word_starts.append(char_pos)\n",
    "            char_pos += len(word)\n",
    "            word_ends.append(char_pos)\n",
    "            char_pos += 1               # WhiteSpace Character\n",
    "\n",
    "        '''\n",
    "        Based on the character limits, the annotations are assigned\n",
    "        A custom IO tagging scheme is used\n",
    "        Labels are assigned on the basis of disease label in annotations\n",
    "        '''\n",
    "\n",
    "        for start, end, disease_info, label in annotations:\n",
    "            for i, (word_start, word_end) in enumerate(zip(word_starts, word_ends)):\n",
    "                if word_start >= start and word_end <= end:\n",
    "                    tags[i] = 'I-' + label\n",
    "                elif word_start < start < word_end or word_start < end < word_end:\n",
    "                    tags[i] = 'I-' + label\n",
    "\n",
    "        tagged_sentences.append((sentence, tags))\n",
    "\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the dataset file\n",
    "lines = read_dataset(train_file)\n",
    "paragraphs = parse_dataset(lines)\n",
    "\n",
    "all_sentences = []\n",
    "all_tags = []\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    sentences, annotations = parse_paragraph(paragraph)\n",
    "    tagged_sentences = tag_annotations(sentences, annotations)\n",
    "    for sentence, tags in tagged_sentences:\n",
    "        all_sentences.append(sentence)\n",
    "        all_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset class\n",
    "class LSTM_Attention_NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, word_encoder, tag_encoder, unknown_token='<UNK>'):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.word_encoder = word_encoder\n",
    "        self.tag_encoder = tag_encoder\n",
    "        self.unknown_token = unknown_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tags = self.tags[idx]\n",
    "\n",
    "        sentence_encoded = [self.word_encoder.get(word, self.word_encoder[self.unknown_token]) for word in sentence]\n",
    "        tags_encoded = self.tag_encoder.transform(tags)\n",
    "\n",
    "        return torch.tensor(sentence_encoded), torch.tensor(tags_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "all_words = [word for sentence in all_sentences for word in sentence]\n",
    "all_tags_flat = [tag for tags in all_tags for tag in tags]\n",
    "\n",
    "word_encoder = {word: idx for idx, word in enumerate(set(all_words))}\n",
    "unknown_token = '<UNK>' \n",
    "word_encoder[unknown_token] = len(word_encoder)  # Add unknown token\n",
    "# Done to prevent KeyError as some words might be out of vocabulary in testing dataset\n",
    "\n",
    "tag_encoder = LabelEncoder()\n",
    "tag_encoder.fit(all_tags_flat)\n",
    "\n",
    "dataset = LSTM_Attention_NERDataset(all_sentences, all_tags, word_encoder, tag_encoder, unknown_token)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Global Attention class\n",
    "class Attention (nn.Module):\n",
    "    def __init__ (self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        # Softmax converts it into a Probability distribution so that the weights are between 0 and 1\n",
    "        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)\n",
    "        weighted_output = lstm_output * attention_weights  # (batch_size, seq_len, hidden_dim)\n",
    "        return weighted_output # .sum(dim=1)  # (batch_size, hidden_dim)\n",
    "\n",
    "class Attention_LSTM_NER_Model(nn.Module):\n",
    "    def __init__ (self, vocab_size, tagset_size, embedding_dim = 128, hidden_dim = 128):\n",
    "        super(Attention_LSTM_NER_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first = True)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "\n",
    "    def forward(self, i):\n",
    "        emb = self.embedding(i)\n",
    "        lstm_out , _ = self.lstm(emb)\n",
    "        att_out = self.attention(lstm_out)\n",
    "        tag_space = self.fc(att_out)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Defining the model characteristics\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = Attention_LSTM_NER_Model(len(word_encoder), len(tag_encoder.classes_)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9774756487210592\n",
      "Epoch 2, Loss: 0.4285265103975932\n",
      "Epoch 3, Loss: 0.3269316228230794\n",
      "Epoch 4, Loss: 0.2948703294992447\n",
      "Epoch 5, Loss: 0.2720580009619395\n",
      "Epoch 6, Loss: 0.25541133830944696\n",
      "Epoch 7, Loss: 0.2438431077202161\n",
      "Epoch 8, Loss: 0.23573602120081583\n",
      "Epoch 9, Loss: 0.23039307355880737\n",
      "Epoch 10, Loss: 0.21865585992733638\n",
      "Epoch 11, Loss: 0.21379994511604308\n",
      "Epoch 12, Loss: 0.20803800026575725\n",
      "Epoch 13, Loss: 0.19900694290796916\n",
      "Epoch 14, Loss: 0.1973389290769895\n",
      "Epoch 15, Loss: 0.19286220868428547\n",
      "Epoch 16, Loss: 0.18842431416114172\n",
      "Epoch 17, Loss: 0.1835669700304667\n",
      "Epoch 18, Loss: 0.1809170419971148\n",
      "Epoch 19, Loss: 0.1812817547718684\n",
      "Epoch 20, Loss: 0.17869124710559844\n",
      "Epoch 21, Loss: 0.17403830389181774\n",
      "Epoch 22, Loss: 0.17145557443300882\n",
      "Epoch 23, Loss: 0.17053409496943156\n",
      "Epoch 24, Loss: 0.17121194064617157\n",
      "Epoch 25, Loss: 0.16669244527816773\n",
      "Epoch 26, Loss: 0.1671228723724683\n",
      "Epoch 27, Loss: 0.16321141501267752\n",
      "Epoch 28, Loss: 0.16038367519776026\n",
      "Epoch 29, Loss: 0.15696417247255642\n",
      "Epoch 30, Loss: 0.15839267234007517\n",
      "Epoch 31, Loss: 0.15584168930848438\n",
      "Epoch 32, Loss: 0.15537736654281617\n",
      "Epoch 33, Loss: 0.15367435415585837\n",
      "Epoch 34, Loss: 0.15404294858376186\n",
      "Epoch 35, Loss: 0.14948328077793122\n",
      "Epoch 36, Loss: 0.153211310505867\n",
      "Epoch 37, Loss: 0.15152751713991164\n",
      "Epoch 38, Loss: 0.15174833655357362\n",
      "Epoch 39, Loss: 0.14814673562844594\n",
      "Epoch 40, Loss: 0.14860588689645132\n",
      "Epoch 41, Loss: 0.1469351942340533\n",
      "Epoch 42, Loss: 0.1481673632065455\n",
      "Epoch 43, Loss: 0.14601644615332285\n",
      "Epoch 44, Loss: 0.1455937800804774\n",
      "Epoch 45, Loss: 0.1429585008819898\n",
      "Epoch 46, Loss: 0.14764196475346883\n",
      "Epoch 47, Loss: 0.1447474562128385\n",
      "Epoch 48, Loss: 0.14310680637756984\n",
      "Epoch 49, Loss: 0.14294229646523793\n",
      "Epoch 50, Loss: 0.1409112372994423\n",
      "Epoch 51, Loss: 0.1379081302881241\n",
      "Epoch 52, Loss: 0.13899257471164067\n",
      "Epoch 53, Loss: 0.1365075586239497\n",
      "Epoch 54, Loss: 0.13535075644652048\n",
      "Epoch 55, Loss: 0.13549130737781526\n",
      "Epoch 56, Loss: 0.1377156713604927\n",
      "Epoch 57, Loss: 0.13463377088308334\n",
      "Epoch 58, Loss: 0.13504777153333028\n",
      "Epoch 59, Loss: 0.13322761783997217\n",
      "Epoch 60, Loss: 0.13741896778345108\n",
      "Epoch 61, Loss: 0.1332925330599149\n",
      "Epoch 62, Loss: 0.13301157146692277\n",
      "Epoch 63, Loss: 0.13361107776562373\n",
      "Epoch 64, Loss: 0.13208830147981643\n",
      "Epoch 65, Loss: 0.13242991576592128\n",
      "Epoch 66, Loss: 0.13442281693220137\n",
      "Epoch 67, Loss: 0.13098265479008356\n",
      "Epoch 68, Loss: 0.1304920252164205\n",
      "Epoch 69, Loss: 0.13115031868219376\n",
      "Epoch 70, Loss: 0.1308184614777565\n",
      "Epoch 71, Loss: 0.1280645087858041\n",
      "Epoch 72, Loss: 0.12882095247507094\n",
      "Epoch 73, Loss: 0.1263466528058052\n",
      "Epoch 74, Loss: 0.12699527541796365\n",
      "Epoch 75, Loss: 0.12477481544017792\n",
      "Epoch 76, Loss: 0.12494820227225621\n",
      "Epoch 77, Loss: 0.12436735401550929\n",
      "Epoch 78, Loss: 0.12417005196213722\n",
      "Epoch 79, Loss: 0.12352198665340741\n",
      "Epoch 80, Loss: 0.12313920358816782\n",
      "Epoch 81, Loss: 0.1229273796081543\n",
      "Epoch 82, Loss: 0.12381705358624458\n",
      "Epoch 83, Loss: 0.12251337210337321\n",
      "Epoch 84, Loss: 0.12533906032641728\n",
      "Epoch 85, Loss: 0.12139417762557665\n",
      "Epoch 86, Loss: 0.12278222451607386\n",
      "Epoch 87, Loss: 0.12056004827221235\n",
      "Epoch 88, Loss: 0.12107236961523692\n",
      "Epoch 89, Loss: 0.12005347659190496\n",
      "Epoch 90, Loss: 0.12016270567973454\n",
      "Epoch 91, Loss: 0.11988643596569697\n",
      "Epoch 92, Loss: 0.12008055239915848\n",
      "Epoch 93, Loss: 0.11784473965565363\n",
      "Epoch 94, Loss: 0.11474084599564473\n",
      "Epoch 95, Loss: 0.11661459634701411\n",
      "Epoch 96, Loss: 0.11579628437757492\n",
      "Epoch 97, Loss: 0.11538174460331599\n",
      "Epoch 98, Loss: 0.11815580040216446\n",
      "Epoch 99, Loss: 0.1164567411939303\n",
      "Epoch 100, Loss: 0.11465373992919922\n",
      "Epoch 101, Loss: 0.11357232297460239\n",
      "Epoch 102, Loss: 0.11619315514961878\n",
      "Epoch 103, Loss: 0.11267023801803588\n",
      "Epoch 104, Loss: 0.1118793959915638\n",
      "Epoch 105, Loss: 0.11297903493046761\n",
      "Epoch 106, Loss: 0.11142608245213827\n",
      "Epoch 107, Loss: 0.11357341373960177\n",
      "Epoch 108, Loss: 0.1119105654458205\n",
      "Epoch 109, Loss: 0.11239921689033508\n",
      "Epoch 110, Loss: 0.11191475798686346\n",
      "Epoch 111, Loss: 0.10819311087330183\n",
      "Epoch 112, Loss: 0.10951869895060858\n",
      "Epoch 113, Loss: 0.10797765170534451\n",
      "Epoch 114, Loss: 0.10780013754963874\n",
      "Epoch 115, Loss: 0.10790897235274315\n",
      "Epoch 116, Loss: 0.10691630666454634\n",
      "Epoch 117, Loss: 0.10599929213523865\n",
      "Epoch 118, Loss: 0.11534817258516948\n",
      "Epoch 119, Loss: 0.11024212802449862\n",
      "Epoch 120, Loss: 0.10840078781048457\n",
      "Epoch 121, Loss: 0.10543165038029353\n",
      "Epoch 122, Loss: 0.10449481005469959\n",
      "Epoch 123, Loss: 0.10459706192215283\n",
      "Epoch 124, Loss: 0.10151129849255085\n",
      "Epoch 125, Loss: 0.09966649825374285\n",
      "Epoch 126, Loss: 0.09971240309377512\n",
      "Epoch 127, Loss: 0.09953949838876724\n",
      "Epoch 128, Loss: 0.09754744768142701\n",
      "Epoch 129, Loss: 0.09803866287072499\n",
      "Epoch 130, Loss: 0.09923143530885378\n",
      "Epoch 131, Loss: 0.09874865268667539\n",
      "Epoch 132, Loss: 0.09942743971943856\n",
      "Epoch 133, Loss: 0.1005105696618557\n",
      "Epoch 134, Loss: 0.09865510145823161\n",
      "Epoch 135, Loss: 0.0988604648411274\n",
      "Epoch 136, Loss: 0.09637418096264204\n",
      "Epoch 137, Loss: 0.09522087280948957\n",
      "Epoch 138, Loss: 0.09285931234558423\n",
      "Epoch 139, Loss: 0.09054578572511673\n",
      "Epoch 140, Loss: 0.09042243073383967\n",
      "Epoch 141, Loss: 0.09024755383531252\n",
      "Epoch 142, Loss: 0.08983385408918063\n",
      "Epoch 143, Loss: 0.08703997604548931\n",
      "Epoch 144, Loss: 0.0844583680232366\n",
      "Epoch 145, Loss: 0.08518779933452607\n",
      "Epoch 146, Loss: 0.08720390863716602\n",
      "Epoch 147, Loss: 0.08978522586325804\n",
      "Epoch 148, Loss: 0.08412272348999977\n",
      "Epoch 149, Loss: 0.08368921806414922\n",
      "Epoch 150, Loss: 0.08167578667402267\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        sentences, tags = zip(*batch)\n",
    "        sentences = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True).to(device)\n",
    "        tags = torch.nn.utils.rnn.pad_sequence(tags, batch_first=True, padding_value=-100).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sentences)\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        loss = criterion(outputs, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model as a .pth file\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "\n",
    "# Load the test dataset\n",
    "test_file = '../../Data/NCBItestset_corpus.txt'\n",
    "test_lines = read_dataset(test_file)\n",
    "test_paragraphs = parse_dataset(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing and storing the test dataset\n",
    "test_sentences = []\n",
    "test_tags = []\n",
    "\n",
    "for paragraph in test_paragraphs:\n",
    "    sentences, annotations = parse_paragraph(paragraph)\n",
    "    tagged_sentences = tag_annotations(sentences, annotations)\n",
    "    for sentence, tags in tagged_sentences:\n",
    "        test_sentences.append(sentence)\n",
    "        test_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention_LSTM_NER_Model(\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (embedding): Embedding(14805, 128)\n",
       "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
       "  (attention): Attention(\n",
       "    (attention): Linear(in_features=128, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the model file\n",
    "model = Attention_LSTM_NER_Model(len(word_encoder), len(tag_encoder.classes_)).to(device)\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "I-CompositeMention       0.00      0.00      0.00        89\n",
      "    I-DiseaseClass       0.20      0.20      0.20       255\n",
      "        I-Modifier       0.42      0.09      0.15       367\n",
      " I-SpecificDisease       0.55      0.63      0.59      1090\n",
      "                 O       0.97      0.98      0.97     18601\n",
      "\n",
      "          accuracy                           0.93     20402\n",
      "         macro avg       0.43      0.38      0.38     20402\n",
      "      weighted avg       0.92      0.93      0.92     20402\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adit\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Adit\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Adit\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Prepare the test data\n",
    "test_dataset = LSTM_Attention_NERDataset(test_sentences, test_tags, word_encoder, tag_encoder, '<UNK>')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "# Evaluate the model\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "result = \"../../Result/TestResults_AttentionLSTM_NER.txt\"\n",
    "with open (result, 'w') as t_file:\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            sentences, tags = zip(*batch)\n",
    "            sentences = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True).to(device)\n",
    "            tags = torch.nn.utils.rnn.pad_sequence(tags, batch_first=True, padding_value=-100).to(device)\n",
    "\n",
    "            outputs = model(sentences)\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            true_labels = tags.cpu().numpy()\n",
    "\n",
    "            mask = true_labels != -100\n",
    "            pred_labels = predictions[mask]\n",
    "            true_labels = true_labels[mask]\n",
    "\n",
    "            pred_labels_decoded = tag_encoder.inverse_transform(pred_labels)\n",
    "            true_labels_decoded = tag_encoder.inverse_transform(true_labels)\n",
    "\n",
    "            for true_label, pred_label in zip(true_labels_decoded, pred_labels_decoded):\n",
    "                t_file.write(f'True: {true_label}, Pred: {pred_label}\\n')\n",
    "                all_true_labels.append(true_label)\n",
    "                all_pred_labels.append(pred_label)\n",
    "\n",
    "# Printing classification report\n",
    "report = classification_report(all_true_labels, all_pred_labels)\n",
    "print (report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
