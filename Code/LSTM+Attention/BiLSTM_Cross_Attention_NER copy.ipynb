{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to implement:\n",
    "<ul>\n",
    "<li>Focal Loss <b>[DONE]</b></li> \n",
    "<li>Undersampling of the \"O\" tags <b>[DONE]</b></li> \n",
    "<li><b>[OPTIONAL]</b> SMOTE with Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from focal_loss import FocalLoss\n",
    "# from imblearn.over_sampliong inport SMOTE\n",
    "# # Note that install both first, then implement\n",
    "from math import sqrt, log\n",
    "\n",
    "# Importing the relevant files\n",
    "train_file = '../../Data/NCBItrainset_corpus.txt'\n",
    "dev_file = '../../Data/NCBIdevelopset_corpus.txt'\n",
    "model_name = '../../Models/BiLSTM_CrossAttention_NER_model.pth'\n",
    "unknown_token = \"<UNK>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and parsing the dataset file\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "def parse_dataset(lines):\n",
    "    paragraphs, paragraph = [], []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            paragraph.append(line)\n",
    "        else:\n",
    "            if paragraph:\n",
    "                paragraphs.append(paragraph)\n",
    "                paragraph = []\n",
    "    if paragraph:\n",
    "        paragraphs.append(paragraph)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paragraph(paragraph):\n",
    "    sentences, annotations, sentence = [], [], []\n",
    "    for line in paragraph:\n",
    "        if re.match(r'^\\d+\\|\\w\\|', line):\n",
    "            sentence.extend(line.split('|')[2].split())\n",
    "        elif re.match(r'^\\d+\\t\\d+\\t\\d+\\t', line):\n",
    "            start, end = int(line.split(\"\\t\")[1]), int(line.split(\"\\t\")[2])\n",
    "            annotations.append((start, end, line.split(\"\\t\")[3], line.split(\"\\t\")[4]))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences, annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging the words based on character limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_annotations(sentences, annotations):\n",
    "    tagged_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tags = ['O'] * len(sentence)\n",
    "        word_starts, word_ends, char_pos = [], [], 0\n",
    "        for word in sentence:\n",
    "            word_starts.append(char_pos)\n",
    "            char_pos += len(word)\n",
    "            word_ends.append(char_pos)\n",
    "            char_pos += 1\n",
    "        for start, end, _, label in annotations:\n",
    "            for i, (word_start, word_end) in enumerate(zip(word_starts, word_ends)):\n",
    "                if word_start >= start and word_end <= end:\n",
    "                    tags[i] = 'I-' + label\n",
    "                elif word_start < start < word_end or word_start < end < word_end:\n",
    "                    tags[i] = 'I-' + label\n",
    "        tagged_sentences.append((sentence, tags))\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training and validation data\n",
    "# Undersampling the outside tagged data\n",
    "lines = read_dataset(train_file)\n",
    "paragraphs = parse_dataset(lines)\n",
    "all_sentences, all_tags = [], []\n",
    "for paragraph in paragraphs:\n",
    "    s, a = parse_paragraph(paragraph)\n",
    "    tagged_sentences = tag_annotations(s, a)\n",
    "    for sentence, tag in tagged_sentences:\n",
    "        all_sentences.append(sentence)\n",
    "        all_tags.append(tag)\n",
    "\n",
    "dev_lines = read_dataset(dev_file)\n",
    "dev_paragraphs = parse_dataset(dev_lines)\n",
    "dev_all_sentences, dev_all_tags = [], []\n",
    "for dev_paragraph in dev_paragraphs:\n",
    "    dev_s, dev_a = parse_paragraph(dev_paragraph)\n",
    "    dev_tagged_sentences = tag_annotations(dev_s, dev_a)\n",
    "    for dev_sentence, dev_tag in dev_tagged_sentences:\n",
    "        dev_all_sentences.append(dev_sentence)\n",
    "        dev_all_tags.append(dev_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT: Modifying the Dataset class to count <UNK> tokens\n",
    "class LSTM_Attention_NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, word_encoder, tag_encoder, unknown_token='<UNK>'): # undersample_ratio = 0.5\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.word_encoder = word_encoder\n",
    "        self.tag_encoder = tag_encoder\n",
    "        self.unknown_token = unknown_token\n",
    "        self.unk_count = 0\n",
    "        self.unk_tags = []\n",
    "        # self.undersample_ratio = undersample_ratio\n",
    "\n",
    "        # # Undersampling of the \"O\" tags\n",
    "        # self.sentences, self.tags = self.undersample()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tags = self.tags[idx]\n",
    "        sentence_encoded = [self.word_encoder.get(word, self.word_encoder[self.unknown_token]) for word in sentence]\n",
    "        \n",
    "        # Count <UNK> tokens and collect their tags\n",
    "        for word, tag in zip(sentence, tags):\n",
    "            if word not in self.word_encoder:\n",
    "                self.unk_count += 1\n",
    "                self.unk_tags.append(tag)\n",
    "        \n",
    "        tags_encoded = self.tag_encoder.transform(tags)\n",
    "        return torch.tensor(sentence_encoded), torch.tensor(tags_encoded, dtype=torch.long)\n",
    "    \n",
    "    # def undersample(self):\n",
    "    #     o_tag_indices = [i for i, tag in enumerate(self.tags) if 'O' in tag]\n",
    "    #     num_o_tags = len(o_tag_indices)\n",
    "\n",
    "    #     num_samples = int(num_o_tags * self.undersample_ratio)\n",
    "    #     sampled_indices = np.random.choice(o_tag_indices, num_samples, replace = False)\n",
    "\n",
    "    #     # Keeping only sampled data\n",
    "    #     sentences_sampled = [self.sentences[i] for i in sampled_indices]\n",
    "    #     tags_sampled = [self.tags[i] for i in sampled_indices]\n",
    "\n",
    "    #     # Combine the sampled data with the non-O tagged data\n",
    "    #     non_o_tags = [i for i in range(len(self.sentences)) if i not in o_tag_indices]\n",
    "    #     sentences_sampled += [self.sentences[i] for i in non_o_tags]\n",
    "    #     tags_sampled += [self.tags[i] for i in non_o_tags]\n",
    "\n",
    "    #     sentences_sampled, tags_sampled = shuffle(sentences_sampled, tags_sampled)\n",
    "    #     return sentences_sampled, tags_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of <UNK> tokens in the training dataset: 0\n",
      "Tags of <UNK> tokens in the training dataset: Counter()\n",
      "Number of <UNK> tokens in the validation dataset: 0\n",
      "Tags of <UNK> tokens in the validation dataset: Counter()\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for sentence in all_sentences for word in sentence]\n",
    "all_tags_flat = [tag for tags in all_tags for tag in tags]\n",
    "\n",
    "word_encoder = {word: idx for idx, word in enumerate(set(all_words))}\n",
    "unknown_token = '<UNK>'\n",
    "word_encoder[unknown_token] = len(word_encoder)\n",
    "\n",
    "tag_encoder = LabelEncoder()\n",
    "tag_encoder.fit(all_tags_flat)\n",
    "\n",
    "# undersample_ratio = 0.5\n",
    "\n",
    "dataset = LSTM_Attention_NERDataset(all_sentences, all_tags, word_encoder, tag_encoder, unknown_token,) #, undersample_ratio)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "dev_dataset = LSTM_Attention_NERDataset(dev_all_sentences, dev_all_tags, word_encoder, tag_encoder, unknown_token) #, undersample_ratio)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: x)\n",
    "\n",
    "# Display the counts and tags of <UNK> tokens\n",
    "print(f\"Number of <UNK> tokens in the training dataset: {dataset.unk_count}\")\n",
    "print(f\"Tags of <UNK> tokens in the training dataset: {Counter(dataset.unk_tags)}\")\n",
    "\n",
    "print(f\"Number of <UNK> tokens in the validation dataset: {dev_dataset.unk_count}\")\n",
    "print(f\"Tags of <UNK> tokens in the validation dataset: {Counter(dev_dataset.unk_tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Attention Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        query_proj = self.query(query)\n",
    "        key_proj = self.key(key)\n",
    "        value_proj = self.value(value)\n",
    "        attention_scores = torch.matmul(query_proj, key_proj.transpose(-2, -1))\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        context = torch.matmul(attention_weights, value_proj)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CrossAttention_NER_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=128, dropout_prob=0.35):\n",
    "        super(BiLSTM_CrossAttention_NER_Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.batch_norm_emb = nn.BatchNorm1d(embedding_dim)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True, dropout=dropout_prob)\n",
    "        self.cross_attention = CrossAttention(hidden_dim * 2)\n",
    "        self.batch_norm_att = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = self.batch_norm_emb(emb.transpose(1, 2)).transpose(1, 2)\n",
    "        emb = self.dropout(emb)\n",
    "        \n",
    "        bilstm_out, _ = self.bilstm(emb)\n",
    "        bilstm_out = self.dropout(bilstm_out)\n",
    "        \n",
    "        cross_att_out = self.cross_attention(bilstm_out, bilstm_out, bilstm_out)\n",
    "        cross_att_out = self.batch_norm_att(cross_att_out.transpose(1, 2)).transpose(1, 2)\n",
    "        cross_att_out = self.dropout(cross_att_out)\n",
    "        \n",
    "        tag_space = self.fc(cross_att_out)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Focal Class (to be used in the case of highly imbalanced datasets of around 1:1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Focal_Loss (nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, ignore_index=-100, reduction = 'mean'):\n",
    "        super(Focal_Loss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction = 'none', ignore_index = self.ignore_index)(outputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        focal_loss = self.alpha * ((1-pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_sentences = list(all_sentences)\n",
    "all_tags = list(all_tags)\n",
    "\n",
    "fold = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting (to be used to visualize the training loss and validation loss)\n",
    "# Used to figure if the mdoel is underfitting or overfitting\n",
    "def graph_plot(title, x_label, y_label, x_data, y_data, z_data, color = 'blue', linestyle = '-'):\n",
    "    plt.plot(x_data, y_data, color = color, linestyle = linestyle)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.plot(x_data, z_data, color = 'r', linestyle = '-')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"../../Graphs/aBiLSTM_CrossAttention.png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6\n",
      "Epoch 1, Average Training Loss: 1.9073, Validation Loss: 0.5766\n",
      "Epoch 2, Average Training Loss: 0.5360, Validation Loss: 0.5057\n",
      "Epoch 3, Average Training Loss: 0.4659, Validation Loss: 0.5157\n",
      "Epoch 4, Average Training Loss: 0.4652, Validation Loss: 0.4952\n",
      "Epoch 5, Average Training Loss: 0.4887, Validation Loss: 0.4983\n",
      "Epoch 6, Average Training Loss: 0.4517, Validation Loss: 0.5110\n",
      "Epoch 7, Average Training Loss: 0.4662, Validation Loss: 0.4847\n",
      "Epoch 8, Average Training Loss: 0.4790, Validation Loss: 0.5055\n",
      "Epoch 9, Average Training Loss: 0.4610, Validation Loss: 0.4825\n",
      "Epoch 10, Average Training Loss: 0.4544, Validation Loss: 0.4973\n",
      "Epoch 11, Average Training Loss: 0.4453, Validation Loss: 0.4965\n",
      "Epoch 12, Average Training Loss: 0.4336, Validation Loss: 0.4952\n",
      "Epoch 13, Average Training Loss: 0.4855, Validation Loss: 0.4892\n",
      "Epoch 14, Average Training Loss: 0.4480, Validation Loss: 0.4843\n",
      "Epoch 15, Average Training Loss: 0.4631, Validation Loss: 0.4831\n",
      "Epoch 16, Average Training Loss: 0.4720, Validation Loss: 0.4761\n",
      "Epoch 17, Average Training Loss: 0.4586, Validation Loss: 0.4752\n",
      "Epoch 18, Average Training Loss: 0.4521, Validation Loss: 0.4888\n",
      "Epoch 19, Average Training Loss: 0.4645, Validation Loss: 0.4895\n",
      "Epoch 20, Average Training Loss: 0.4742, Validation Loss: 0.4801\n",
      "Epoch 21, Average Training Loss: 0.4571, Validation Loss: 0.4796\n",
      "Epoch 22, Average Training Loss: 0.4479, Validation Loss: 0.4834\n",
      "Epoch 23, Average Training Loss: 0.4501, Validation Loss: 0.4814\n",
      "Epoch 24, Average Training Loss: 0.4527, Validation Loss: 0.4819\n",
      "Epoch 25, Average Training Loss: 0.4703, Validation Loss: 0.4826\n",
      "Epoch 26, Average Training Loss: 0.4399, Validation Loss: 0.4827\n",
      "Epoch 27, Average Training Loss: 0.4548, Validation Loss: 0.4830\n",
      "Epoch 28, Average Training Loss: 0.4561, Validation Loss: 0.4807\n",
      "Epoch 29, Average Training Loss: 0.4248, Validation Loss: 0.4839\n",
      "Epoch 30, Average Training Loss: 0.4399, Validation Loss: 0.4802\n",
      "Epoch 31, Average Training Loss: 0.4457, Validation Loss: 0.4831\n",
      "Epoch 32, Average Training Loss: 0.4404, Validation Loss: 0.4806\n",
      "Epoch 33, Average Training Loss: 0.4419, Validation Loss: 0.4822\n",
      "Epoch 34, Average Training Loss: 0.4611, Validation Loss: 0.4800\n",
      "Epoch 35, Average Training Loss: 0.4469, Validation Loss: 0.4818\n",
      "Epoch 36, Average Training Loss: 0.4291, Validation Loss: 0.4816\n",
      "Epoch 37, Average Training Loss: 0.4390, Validation Loss: 0.4853\n",
      "Epoch 38, Average Training Loss: 0.4530, Validation Loss: 0.4792\n",
      "Epoch 39, Average Training Loss: 0.4545, Validation Loss: 0.4812\n",
      "Epoch 40, Average Training Loss: 0.4227, Validation Loss: 0.4880\n",
      "Epoch 41, Average Training Loss: 0.4512, Validation Loss: 0.4819\n",
      "Epoch 42, Average Training Loss: 0.4239, Validation Loss: 0.4841\n",
      "Epoch 43, Average Training Loss: 0.4467, Validation Loss: 0.4824\n",
      "Epoch 44, Average Training Loss: 0.4379, Validation Loss: 0.4823\n",
      "Epoch 45, Average Training Loss: 0.4568, Validation Loss: 0.4808\n",
      "Epoch 46, Average Training Loss: 0.4335, Validation Loss: 0.4817\n",
      "Epoch 47, Average Training Loss: 0.4581, Validation Loss: 0.4796\n",
      "Epoch 48, Average Training Loss: 0.4452, Validation Loss: 0.4807\n",
      "Epoch 49, Average Training Loss: 0.4953, Validation Loss: 0.4805\n",
      "Epoch 50, Average Training Loss: 0.4345, Validation Loss: 0.4802\n",
      "Epoch 51, Average Training Loss: 0.4325, Validation Loss: 0.4798\n",
      "Epoch 52, Average Training Loss: 0.4507, Validation Loss: 0.4835\n",
      "Epoch 53, Average Training Loss: 0.4428, Validation Loss: 0.4823\n",
      "Epoch 54, Average Training Loss: 0.4499, Validation Loss: 0.4809\n",
      "Epoch 55, Average Training Loss: 0.4605, Validation Loss: 0.4836\n",
      "Epoch 56, Average Training Loss: 0.4381, Validation Loss: 0.4806\n",
      "Epoch 57, Average Training Loss: 0.4464, Validation Loss: 0.4809\n",
      "Epoch 58, Average Training Loss: 0.4590, Validation Loss: 0.4830\n",
      "Epoch 59, Average Training Loss: 0.4426, Validation Loss: 0.4830\n",
      "Epoch 60, Average Training Loss: 0.4585, Validation Loss: 0.4818\n",
      "Epoch 61, Average Training Loss: 0.4228, Validation Loss: 0.4850\n",
      "Epoch 62, Average Training Loss: 0.4506, Validation Loss: 0.4805\n",
      "Epoch 63, Average Training Loss: 0.4446, Validation Loss: 0.4827\n",
      "Epoch 64, Average Training Loss: 0.4195, Validation Loss: 0.4805\n",
      "Epoch 65, Average Training Loss: 0.4444, Validation Loss: 0.4832\n",
      "Epoch 66, Average Training Loss: 0.4448, Validation Loss: 0.4827\n",
      "Epoch 67, Average Training Loss: 0.4343, Validation Loss: 0.4843\n",
      "Epoch 68, Average Training Loss: 0.4450, Validation Loss: 0.4810\n",
      "Epoch 69, Average Training Loss: 0.4416, Validation Loss: 0.4816\n",
      "Epoch 70, Average Training Loss: 0.4656, Validation Loss: 0.4836\n",
      "Epoch 71, Average Training Loss: 0.4451, Validation Loss: 0.4840\n",
      "Epoch 72, Average Training Loss: 0.4633, Validation Loss: 0.4810\n",
      "Epoch 73, Average Training Loss: 0.4229, Validation Loss: 0.4806\n",
      "Epoch 74, Average Training Loss: 0.4305, Validation Loss: 0.4813\n",
      "Epoch 75, Average Training Loss: 0.4563, Validation Loss: 0.4814\n",
      "Epoch 76, Average Training Loss: 0.4303, Validation Loss: 0.4789\n",
      "Epoch 77, Average Training Loss: 0.4352, Validation Loss: 0.4841\n",
      "Epoch 78, Average Training Loss: 0.4467, Validation Loss: 0.4805\n",
      "Epoch 79, Average Training Loss: 0.4359, Validation Loss: 0.4819\n",
      "Epoch 80, Average Training Loss: 0.4522, Validation Loss: 0.4824\n",
      "Fold 7\n",
      "Epoch 1, Average Training Loss: 1.7259, Validation Loss: 0.5224\n",
      "Epoch 2, Average Training Loss: 0.4942, Validation Loss: 0.4856\n",
      "Epoch 3, Average Training Loss: 0.4919, Validation Loss: 0.4711\n",
      "Epoch 4, Average Training Loss: 0.4638, Validation Loss: 0.4716\n",
      "Epoch 5, Average Training Loss: 0.4469, Validation Loss: 0.4640\n",
      "Epoch 6, Average Training Loss: 0.4632, Validation Loss: 0.4627\n",
      "Epoch 7, Average Training Loss: 0.4651, Validation Loss: 0.4574\n",
      "Epoch 8, Average Training Loss: 0.4464, Validation Loss: 0.4680\n",
      "Epoch 9, Average Training Loss: 0.4441, Validation Loss: 0.4592\n",
      "Epoch 10, Average Training Loss: 0.4663, Validation Loss: 0.4573\n",
      "Epoch 11, Average Training Loss: 0.4416, Validation Loss: 0.4635\n",
      "Epoch 12, Average Training Loss: 0.4504, Validation Loss: 0.4504\n",
      "Epoch 13, Average Training Loss: 0.4333, Validation Loss: 0.4635\n",
      "Epoch 14, Average Training Loss: 0.4319, Validation Loss: 0.4454\n",
      "Epoch 15, Average Training Loss: 0.4169, Validation Loss: 0.4450\n",
      "Epoch 16, Average Training Loss: 0.4080, Validation Loss: 0.4383\n",
      "Epoch 17, Average Training Loss: 0.3886, Validation Loss: 0.4289\n",
      "Epoch 18, Average Training Loss: 0.3737, Validation Loss: 0.4148\n",
      "Epoch 19, Average Training Loss: 0.3836, Validation Loss: 0.4066\n",
      "Epoch 20, Average Training Loss: 0.3555, Validation Loss: 0.3997\n",
      "Epoch 21, Average Training Loss: 0.3738, Validation Loss: 0.3951\n",
      "Epoch 22, Average Training Loss: 0.3453, Validation Loss: 0.3799\n",
      "Epoch 23, Average Training Loss: 0.3212, Validation Loss: 0.3842\n",
      "Epoch 24, Average Training Loss: 0.3159, Validation Loss: 0.3736\n",
      "Epoch 25, Average Training Loss: 0.3007, Validation Loss: 0.3648\n",
      "Epoch 26, Average Training Loss: 0.3007, Validation Loss: 0.3586\n",
      "Epoch 27, Average Training Loss: 0.3000, Validation Loss: 0.3539\n",
      "Epoch 28, Average Training Loss: 0.2779, Validation Loss: 0.3433\n",
      "Epoch 29, Average Training Loss: 0.2806, Validation Loss: 0.3461\n",
      "Epoch 30, Average Training Loss: 0.2526, Validation Loss: 0.3364\n",
      "Epoch 31, Average Training Loss: 0.2577, Validation Loss: 0.3328\n",
      "Epoch 32, Average Training Loss: 0.2523, Validation Loss: 0.3338\n",
      "Epoch 33, Average Training Loss: 0.2346, Validation Loss: 0.3177\n",
      "Epoch 34, Average Training Loss: 0.2448, Validation Loss: 0.3127\n",
      "Epoch 35, Average Training Loss: 0.2099, Validation Loss: 0.3150\n",
      "Epoch 36, Average Training Loss: 0.2127, Validation Loss: 0.3057\n",
      "Epoch 37, Average Training Loss: 0.2032, Validation Loss: 0.3161\n",
      "Epoch 38, Average Training Loss: 0.2024, Validation Loss: 0.3050\n",
      "Epoch 39, Average Training Loss: 0.1994, Validation Loss: 0.2976\n",
      "Epoch 40, Average Training Loss: 0.1923, Validation Loss: 0.3001\n",
      "Epoch 41, Average Training Loss: 0.1856, Validation Loss: 0.2915\n",
      "Epoch 42, Average Training Loss: 0.1706, Validation Loss: 0.2907\n",
      "Epoch 43, Average Training Loss: 0.1748, Validation Loss: 0.2924\n",
      "Epoch 44, Average Training Loss: 0.1787, Validation Loss: 0.2875\n",
      "Epoch 45, Average Training Loss: 0.1737, Validation Loss: 0.2879\n",
      "Epoch 46, Average Training Loss: 0.1607, Validation Loss: 0.2785\n",
      "Epoch 47, Average Training Loss: 0.1570, Validation Loss: 0.2775\n",
      "Epoch 48, Average Training Loss: 0.1460, Validation Loss: 0.2727\n",
      "Epoch 49, Average Training Loss: 0.1463, Validation Loss: 0.2738\n",
      "Epoch 50, Average Training Loss: 0.1537, Validation Loss: 0.2802\n",
      "Epoch 51, Average Training Loss: 0.1379, Validation Loss: 0.2808\n",
      "Epoch 52, Average Training Loss: 0.1388, Validation Loss: 0.2606\n",
      "Epoch 53, Average Training Loss: 0.1367, Validation Loss: 0.2672\n",
      "Epoch 54, Average Training Loss: 0.1388, Validation Loss: 0.2673\n",
      "Epoch 55, Average Training Loss: 0.1266, Validation Loss: 0.2605\n",
      "Epoch 56, Average Training Loss: 0.1340, Validation Loss: 0.2713\n",
      "Epoch 57, Average Training Loss: 0.1168, Validation Loss: 0.2645\n",
      "Epoch 58, Average Training Loss: 0.1169, Validation Loss: 0.2613\n",
      "Epoch 59, Average Training Loss: 0.1161, Validation Loss: 0.2669\n",
      "Epoch 60, Average Training Loss: 0.1133, Validation Loss: 0.2581\n",
      "Epoch 61, Average Training Loss: 0.1074, Validation Loss: 0.2688\n",
      "Epoch 62, Average Training Loss: 0.1103, Validation Loss: 0.2559\n",
      "Epoch 63, Average Training Loss: 0.1026, Validation Loss: 0.2554\n",
      "Epoch 64, Average Training Loss: 0.0987, Validation Loss: 0.2626\n",
      "Epoch 65, Average Training Loss: 0.1033, Validation Loss: 0.2637\n",
      "Epoch 66, Average Training Loss: 0.0981, Validation Loss: 0.2638\n",
      "Epoch 67, Average Training Loss: 0.1016, Validation Loss: 0.2661\n",
      "Epoch 68, Average Training Loss: 0.0943, Validation Loss: 0.2620\n",
      "Epoch 69, Average Training Loss: 0.0944, Validation Loss: 0.2597\n",
      "Epoch 70, Average Training Loss: 0.0942, Validation Loss: 0.2575\n",
      "Epoch 71, Average Training Loss: 0.0973, Validation Loss: 0.2653\n",
      "Epoch 72, Average Training Loss: 0.0893, Validation Loss: 0.2689\n",
      "Epoch 73, Average Training Loss: 0.0864, Validation Loss: 0.2648\n",
      "Epoch 74, Average Training Loss: 0.0902, Validation Loss: 0.2591\n",
      "Epoch 75, Average Training Loss: 0.0847, Validation Loss: 0.2752\n",
      "Epoch 76, Average Training Loss: 0.0862, Validation Loss: 0.2610\n",
      "Epoch 77, Average Training Loss: 0.0829, Validation Loss: 0.2536\n",
      "Epoch 78, Average Training Loss: 0.0760, Validation Loss: 0.2593\n",
      "Epoch 79, Average Training Loss: 0.0795, Validation Loss: 0.2470\n",
      "Epoch 80, Average Training Loss: 0.0720, Validation Loss: 0.2647\n",
      "Fold 8\n",
      "Epoch 1, Average Training Loss: 1.1927, Validation Loss: 0.5614\n",
      "Epoch 2, Average Training Loss: 0.4574, Validation Loss: 0.5121\n",
      "Epoch 3, Average Training Loss: 0.4528, Validation Loss: 0.5126\n",
      "Epoch 4, Average Training Loss: 0.4905, Validation Loss: 0.5082\n",
      "Epoch 5, Average Training Loss: 0.4433, Validation Loss: 0.5091\n",
      "Epoch 6, Average Training Loss: 0.4350, Validation Loss: 0.5008\n",
      "Epoch 7, Average Training Loss: 0.4272, Validation Loss: 0.4931\n",
      "Epoch 8, Average Training Loss: 0.4373, Validation Loss: 0.4965\n",
      "Epoch 9, Average Training Loss: 0.4116, Validation Loss: 0.4942\n",
      "Epoch 10, Average Training Loss: 0.4066, Validation Loss: 0.4893\n",
      "Epoch 11, Average Training Loss: 0.4065, Validation Loss: 0.4845\n",
      "Epoch 12, Average Training Loss: 0.4136, Validation Loss: 0.4794\n",
      "Epoch 13, Average Training Loss: 0.3964, Validation Loss: 0.4705\n",
      "Epoch 14, Average Training Loss: 0.3820, Validation Loss: 0.4573\n",
      "Epoch 15, Average Training Loss: 0.3560, Validation Loss: 0.4453\n",
      "Epoch 16, Average Training Loss: 0.3769, Validation Loss: 0.4385\n",
      "Epoch 17, Average Training Loss: 0.3888, Validation Loss: 0.4397\n",
      "Epoch 18, Average Training Loss: 0.3649, Validation Loss: 0.4395\n",
      "Epoch 19, Average Training Loss: 0.3613, Validation Loss: 0.4363\n",
      "Epoch 20, Average Training Loss: 0.3386, Validation Loss: 0.4142\n",
      "Epoch 21, Average Training Loss: 0.3284, Validation Loss: 0.4113\n",
      "Epoch 22, Average Training Loss: 0.3183, Validation Loss: 0.3983\n",
      "Epoch 23, Average Training Loss: 0.3233, Validation Loss: 0.3973\n",
      "Epoch 24, Average Training Loss: 0.3161, Validation Loss: 0.3942\n",
      "Epoch 25, Average Training Loss: 0.3073, Validation Loss: 0.3884\n",
      "Epoch 26, Average Training Loss: 0.3084, Validation Loss: 0.3911\n",
      "Epoch 27, Average Training Loss: 0.3079, Validation Loss: 0.3738\n",
      "Epoch 28, Average Training Loss: 0.2942, Validation Loss: 0.3815\n",
      "Epoch 29, Average Training Loss: 0.2787, Validation Loss: 0.3702\n",
      "Epoch 30, Average Training Loss: 0.2768, Validation Loss: 0.3681\n",
      "Epoch 31, Average Training Loss: 0.2683, Validation Loss: 0.3658\n",
      "Epoch 32, Average Training Loss: 0.2629, Validation Loss: 0.3546\n",
      "Epoch 33, Average Training Loss: 0.2854, Validation Loss: 0.3555\n",
      "Epoch 34, Average Training Loss: 0.2702, Validation Loss: 0.3535\n",
      "Epoch 35, Average Training Loss: 0.2697, Validation Loss: 0.3589\n",
      "Epoch 36, Average Training Loss: 0.2681, Validation Loss: 0.3492\n",
      "Epoch 37, Average Training Loss: 0.2548, Validation Loss: 0.3533\n",
      "Epoch 38, Average Training Loss: 0.2572, Validation Loss: 0.3447\n",
      "Epoch 39, Average Training Loss: 0.2659, Validation Loss: 0.3401\n",
      "Epoch 40, Average Training Loss: 0.2430, Validation Loss: 0.3426\n",
      "Epoch 41, Average Training Loss: 0.2427, Validation Loss: 0.3441\n",
      "Epoch 42, Average Training Loss: 0.2328, Validation Loss: 0.3364\n",
      "Epoch 43, Average Training Loss: 0.2488, Validation Loss: 0.3322\n",
      "Epoch 44, Average Training Loss: 0.2308, Validation Loss: 0.3373\n",
      "Epoch 45, Average Training Loss: 0.2391, Validation Loss: 0.3393\n",
      "Epoch 46, Average Training Loss: 0.2378, Validation Loss: 0.3432\n",
      "Epoch 47, Average Training Loss: 0.2380, Validation Loss: 0.3323\n",
      "Epoch 48, Average Training Loss: 0.2429, Validation Loss: 0.3333\n",
      "Epoch 49, Average Training Loss: 0.2263, Validation Loss: 0.3352\n",
      "Epoch 50, Average Training Loss: 0.2356, Validation Loss: 0.3398\n",
      "Epoch 51, Average Training Loss: 0.2366, Validation Loss: 0.3310\n",
      "Epoch 52, Average Training Loss: 0.2248, Validation Loss: 0.3338\n",
      "Epoch 53, Average Training Loss: 0.2248, Validation Loss: 0.3311\n",
      "Epoch 54, Average Training Loss: 0.2334, Validation Loss: 0.3322\n",
      "Epoch 55, Average Training Loss: 0.2383, Validation Loss: 0.3324\n",
      "Epoch 56, Average Training Loss: 0.2270, Validation Loss: 0.3359\n",
      "Epoch 57, Average Training Loss: 0.2391, Validation Loss: 0.3257\n",
      "Epoch 58, Average Training Loss: 0.2161, Validation Loss: 0.3272\n",
      "Epoch 59, Average Training Loss: 0.2218, Validation Loss: 0.3314\n",
      "Epoch 60, Average Training Loss: 0.2242, Validation Loss: 0.3303\n",
      "Epoch 61, Average Training Loss: 0.2168, Validation Loss: 0.3262\n",
      "Epoch 62, Average Training Loss: 0.2124, Validation Loss: 0.3365\n",
      "Epoch 63, Average Training Loss: 0.2289, Validation Loss: 0.3246\n",
      "Epoch 64, Average Training Loss: 0.2329, Validation Loss: 0.3290\n",
      "Epoch 65, Average Training Loss: 0.2294, Validation Loss: 0.3301\n",
      "Epoch 66, Average Training Loss: 0.2191, Validation Loss: 0.3255\n",
      "Epoch 67, Average Training Loss: 0.2243, Validation Loss: 0.3292\n",
      "Epoch 68, Average Training Loss: 0.2195, Validation Loss: 0.3270\n",
      "Epoch 69, Average Training Loss: 0.2171, Validation Loss: 0.3272\n",
      "Epoch 70, Average Training Loss: 0.2268, Validation Loss: 0.3218\n",
      "Epoch 71, Average Training Loss: 0.2297, Validation Loss: 0.3323\n",
      "Epoch 72, Average Training Loss: 0.2370, Validation Loss: 0.3270\n",
      "Epoch 73, Average Training Loss: 0.2323, Validation Loss: 0.3286\n",
      "Epoch 74, Average Training Loss: 0.2255, Validation Loss: 0.3225\n",
      "Epoch 75, Average Training Loss: 0.2187, Validation Loss: 0.3299\n",
      "Epoch 76, Average Training Loss: 0.2215, Validation Loss: 0.3248\n",
      "Epoch 77, Average Training Loss: 0.2201, Validation Loss: 0.3253\n",
      "Epoch 78, Average Training Loss: 0.2247, Validation Loss: 0.3343\n",
      "Epoch 79, Average Training Loss: 0.2166, Validation Loss: 0.3237\n",
      "Epoch 80, Average Training Loss: 0.2095, Validation Loss: 0.3229\n",
      "Fold 9\n",
      "Epoch 1, Average Training Loss: 2.2298, Validation Loss: 0.5119\n",
      "Epoch 2, Average Training Loss: 0.5473, Validation Loss: 0.4623\n",
      "Epoch 3, Average Training Loss: 0.4804, Validation Loss: 0.4574\n",
      "Epoch 4, Average Training Loss: 0.4762, Validation Loss: 0.4581\n",
      "Epoch 5, Average Training Loss: 0.4534, Validation Loss: 0.4446\n",
      "Epoch 6, Average Training Loss: 0.4571, Validation Loss: 0.4457\n",
      "Epoch 7, Average Training Loss: 0.4410, Validation Loss: 0.4499\n",
      "Epoch 8, Average Training Loss: 0.4774, Validation Loss: 0.4529\n",
      "Epoch 9, Average Training Loss: 0.4670, Validation Loss: 0.4439\n",
      "Epoch 10, Average Training Loss: 0.4619, Validation Loss: 0.4418\n",
      "Epoch 11, Average Training Loss: 0.4593, Validation Loss: 0.4374\n",
      "Epoch 12, Average Training Loss: 0.4338, Validation Loss: 0.4368\n",
      "Epoch 13, Average Training Loss: 0.4674, Validation Loss: 0.4390\n",
      "Epoch 14, Average Training Loss: 0.4320, Validation Loss: 0.4267\n",
      "Epoch 15, Average Training Loss: 0.4578, Validation Loss: 0.4353\n",
      "Epoch 16, Average Training Loss: 0.4396, Validation Loss: 0.4286\n",
      "Epoch 17, Average Training Loss: 0.4499, Validation Loss: 0.4302\n",
      "Epoch 18, Average Training Loss: 0.4387, Validation Loss: 0.4293\n",
      "Epoch 19, Average Training Loss: 0.4231, Validation Loss: 0.4247\n",
      "Epoch 20, Average Training Loss: 0.4216, Validation Loss: 0.4264\n",
      "Epoch 21, Average Training Loss: 0.4529, Validation Loss: 0.4193\n",
      "Epoch 22, Average Training Loss: 0.4229, Validation Loss: 0.4259\n",
      "Epoch 23, Average Training Loss: 0.4353, Validation Loss: 0.4163\n",
      "Epoch 24, Average Training Loss: 0.4133, Validation Loss: 0.4176\n",
      "Epoch 25, Average Training Loss: 0.4296, Validation Loss: 0.4178\n",
      "Epoch 26, Average Training Loss: 0.4265, Validation Loss: 0.4139\n",
      "Epoch 27, Average Training Loss: 0.4006, Validation Loss: 0.4085\n",
      "Epoch 28, Average Training Loss: 0.3997, Validation Loss: 0.4107\n",
      "Epoch 29, Average Training Loss: 0.4305, Validation Loss: 0.4122\n",
      "Epoch 30, Average Training Loss: 0.3927, Validation Loss: 0.4135\n",
      "Epoch 31, Average Training Loss: 0.4165, Validation Loss: 0.4076\n",
      "Epoch 32, Average Training Loss: 0.3789, Validation Loss: 0.4019\n",
      "Epoch 33, Average Training Loss: 0.4001, Validation Loss: 0.4060\n",
      "Epoch 34, Average Training Loss: 0.4040, Validation Loss: 0.4029\n",
      "Epoch 35, Average Training Loss: 0.4173, Validation Loss: 0.3999\n",
      "Epoch 36, Average Training Loss: 0.3737, Validation Loss: 0.3969\n",
      "Epoch 37, Average Training Loss: 0.3972, Validation Loss: 0.3984\n",
      "Epoch 38, Average Training Loss: 0.3978, Validation Loss: 0.3973\n",
      "Epoch 39, Average Training Loss: 0.3811, Validation Loss: 0.3924\n",
      "Epoch 40, Average Training Loss: 0.3769, Validation Loss: 0.3902\n",
      "Epoch 41, Average Training Loss: 0.3762, Validation Loss: 0.3891\n",
      "Epoch 42, Average Training Loss: 0.3729, Validation Loss: 0.3874\n",
      "Epoch 43, Average Training Loss: 0.3678, Validation Loss: 0.3854\n",
      "Epoch 44, Average Training Loss: 0.3839, Validation Loss: 0.3900\n",
      "Epoch 45, Average Training Loss: 0.3716, Validation Loss: 0.3818\n",
      "Epoch 46, Average Training Loss: 0.3674, Validation Loss: 0.3892\n",
      "Epoch 47, Average Training Loss: 0.3740, Validation Loss: 0.3841\n",
      "Epoch 48, Average Training Loss: 0.3806, Validation Loss: 0.3827\n",
      "Epoch 49, Average Training Loss: 0.3617, Validation Loss: 0.3793\n",
      "Epoch 50, Average Training Loss: 0.3732, Validation Loss: 0.3786\n",
      "Epoch 51, Average Training Loss: 0.3685, Validation Loss: 0.3799\n",
      "Epoch 52, Average Training Loss: 0.3507, Validation Loss: 0.3717\n",
      "Epoch 53, Average Training Loss: 0.3768, Validation Loss: 0.3758\n",
      "Epoch 54, Average Training Loss: 0.3549, Validation Loss: 0.3763\n",
      "Epoch 55, Average Training Loss: 0.3487, Validation Loss: 0.3769\n",
      "Epoch 56, Average Training Loss: 0.3522, Validation Loss: 0.3773\n",
      "Epoch 57, Average Training Loss: 0.3432, Validation Loss: 0.3701\n",
      "Epoch 58, Average Training Loss: 0.3521, Validation Loss: 0.3759\n",
      "Epoch 59, Average Training Loss: 0.3520, Validation Loss: 0.3685\n",
      "Epoch 60, Average Training Loss: 0.3447, Validation Loss: 0.3685\n",
      "Epoch 61, Average Training Loss: 0.3400, Validation Loss: 0.3758\n",
      "Epoch 62, Average Training Loss: 0.3562, Validation Loss: 0.3663\n",
      "Epoch 63, Average Training Loss: 0.3469, Validation Loss: 0.3763\n",
      "Epoch 64, Average Training Loss: 0.3517, Validation Loss: 0.3704\n",
      "Epoch 65, Average Training Loss: 0.3687, Validation Loss: 0.3720\n",
      "Epoch 66, Average Training Loss: 0.3535, Validation Loss: 0.3719\n",
      "Epoch 67, Average Training Loss: 0.3349, Validation Loss: 0.3698\n",
      "Epoch 68, Average Training Loss: 0.3402, Validation Loss: 0.3697\n",
      "Epoch 69, Average Training Loss: 0.3379, Validation Loss: 0.3646\n",
      "Epoch 70, Average Training Loss: 0.3365, Validation Loss: 0.3672\n",
      "Epoch 71, Average Training Loss: 0.3384, Validation Loss: 0.3749\n",
      "Epoch 72, Average Training Loss: 0.3434, Validation Loss: 0.3632\n",
      "Epoch 73, Average Training Loss: 0.3250, Validation Loss: 0.3667\n",
      "Epoch 74, Average Training Loss: 0.3373, Validation Loss: 0.3674\n",
      "Epoch 75, Average Training Loss: 0.3359, Validation Loss: 0.3637\n",
      "Epoch 76, Average Training Loss: 0.3308, Validation Loss: 0.3713\n",
      "Epoch 77, Average Training Loss: 0.3323, Validation Loss: 0.3656\n",
      "Epoch 78, Average Training Loss: 0.3490, Validation Loss: 0.3660\n",
      "Epoch 79, Average Training Loss: 0.3495, Validation Loss: 0.3650\n",
      "Epoch 80, Average Training Loss: 0.3259, Validation Loss: 0.3662\n",
      "Fold 10\n",
      "Epoch 1, Average Training Loss: 1.5456, Validation Loss: 0.5013\n",
      "Epoch 2, Average Training Loss: 0.4944, Validation Loss: 0.4329\n",
      "Epoch 3, Average Training Loss: 0.4659, Validation Loss: 0.4344\n",
      "Epoch 4, Average Training Loss: 0.4518, Validation Loss: 0.4170\n",
      "Epoch 5, Average Training Loss: 0.4668, Validation Loss: 0.4154\n",
      "Epoch 6, Average Training Loss: 0.4389, Validation Loss: 0.4091\n",
      "Epoch 7, Average Training Loss: 0.4487, Validation Loss: 0.4126\n",
      "Epoch 8, Average Training Loss: 0.4672, Validation Loss: 0.4243\n",
      "Epoch 9, Average Training Loss: 0.4745, Validation Loss: 0.4110\n",
      "Epoch 10, Average Training Loss: 0.4452, Validation Loss: 0.4206\n",
      "Epoch 11, Average Training Loss: 0.4378, Validation Loss: 0.4049\n",
      "Epoch 12, Average Training Loss: 0.4459, Validation Loss: 0.4076\n",
      "Epoch 13, Average Training Loss: 0.4621, Validation Loss: 0.4053\n",
      "Epoch 14, Average Training Loss: 0.4330, Validation Loss: 0.3945\n",
      "Epoch 15, Average Training Loss: 0.4236, Validation Loss: 0.3935\n",
      "Epoch 16, Average Training Loss: 0.4130, Validation Loss: 0.3919\n",
      "Epoch 17, Average Training Loss: 0.4193, Validation Loss: 0.4039\n",
      "Epoch 18, Average Training Loss: 0.4351, Validation Loss: 0.3895\n",
      "Epoch 19, Average Training Loss: 0.4333, Validation Loss: 0.4029\n",
      "Epoch 20, Average Training Loss: 0.4330, Validation Loss: 0.3844\n",
      "Epoch 21, Average Training Loss: 0.4009, Validation Loss: 0.3846\n",
      "Epoch 22, Average Training Loss: 0.4420, Validation Loss: 0.3933\n",
      "Epoch 23, Average Training Loss: 0.3980, Validation Loss: 0.3823\n",
      "Epoch 24, Average Training Loss: 0.3967, Validation Loss: 0.3742\n",
      "Epoch 25, Average Training Loss: 0.3993, Validation Loss: 0.3791\n",
      "Epoch 26, Average Training Loss: 0.4049, Validation Loss: 0.3738\n",
      "Epoch 27, Average Training Loss: 0.4135, Validation Loss: 0.3824\n",
      "Epoch 28, Average Training Loss: 0.4018, Validation Loss: 0.3711\n",
      "Epoch 29, Average Training Loss: 0.4006, Validation Loss: 0.3728\n",
      "Epoch 30, Average Training Loss: 0.3968, Validation Loss: 0.3709\n",
      "Epoch 31, Average Training Loss: 0.3987, Validation Loss: 0.3663\n",
      "Epoch 32, Average Training Loss: 0.3959, Validation Loss: 0.3636\n",
      "Epoch 33, Average Training Loss: 0.3818, Validation Loss: 0.3726\n",
      "Epoch 34, Average Training Loss: 0.3841, Validation Loss: 0.3679\n",
      "Epoch 35, Average Training Loss: 0.3892, Validation Loss: 0.3671\n",
      "Epoch 36, Average Training Loss: 0.3851, Validation Loss: 0.3686\n",
      "Epoch 37, Average Training Loss: 0.3887, Validation Loss: 0.3644\n",
      "Epoch 38, Average Training Loss: 0.3925, Validation Loss: 0.3669\n",
      "Epoch 39, Average Training Loss: 0.3717, Validation Loss: 0.3680\n",
      "Epoch 40, Average Training Loss: 0.3893, Validation Loss: 0.3646\n",
      "Epoch 41, Average Training Loss: 0.3724, Validation Loss: 0.3666\n",
      "Epoch 42, Average Training Loss: 0.3707, Validation Loss: 0.3585\n",
      "Epoch 43, Average Training Loss: 0.3752, Validation Loss: 0.3609\n",
      "Epoch 44, Average Training Loss: 0.3592, Validation Loss: 0.3637\n",
      "Epoch 45, Average Training Loss: 0.3690, Validation Loss: 0.3676\n",
      "Epoch 46, Average Training Loss: 0.3812, Validation Loss: 0.3654\n",
      "Epoch 47, Average Training Loss: 0.3716, Validation Loss: 0.3623\n",
      "Epoch 48, Average Training Loss: 0.3658, Validation Loss: 0.3568\n",
      "Epoch 49, Average Training Loss: 0.3880, Validation Loss: 0.3662\n",
      "Epoch 50, Average Training Loss: 0.3697, Validation Loss: 0.3635\n",
      "Epoch 51, Average Training Loss: 0.3730, Validation Loss: 0.3599\n",
      "Epoch 52, Average Training Loss: 0.3692, Validation Loss: 0.3591\n",
      "Epoch 53, Average Training Loss: 0.3691, Validation Loss: 0.3670\n",
      "Epoch 54, Average Training Loss: 0.3690, Validation Loss: 0.3631\n",
      "Epoch 55, Average Training Loss: 0.3656, Validation Loss: 0.3667\n",
      "Epoch 56, Average Training Loss: 0.3657, Validation Loss: 0.3555\n",
      "Epoch 57, Average Training Loss: 0.3748, Validation Loss: 0.3660\n",
      "Epoch 58, Average Training Loss: 0.3849, Validation Loss: 0.3602\n",
      "Epoch 59, Average Training Loss: 0.3971, Validation Loss: 0.3623\n",
      "Epoch 60, Average Training Loss: 0.3704, Validation Loss: 0.3589\n",
      "Epoch 61, Average Training Loss: 0.3701, Validation Loss: 0.3684\n",
      "Epoch 62, Average Training Loss: 0.3862, Validation Loss: 0.3599\n",
      "Epoch 63, Average Training Loss: 0.3637, Validation Loss: 0.3652\n",
      "Epoch 64, Average Training Loss: 0.3922, Validation Loss: 0.3567\n",
      "Epoch 65, Average Training Loss: 0.3782, Validation Loss: 0.3597\n",
      "Epoch 66, Average Training Loss: 0.3668, Validation Loss: 0.3638\n",
      "Epoch 67, Average Training Loss: 0.3882, Validation Loss: 0.3595\n",
      "Epoch 68, Average Training Loss: 0.3748, Validation Loss: 0.3604\n",
      "Epoch 69, Average Training Loss: 0.3800, Validation Loss: 0.3615\n",
      "Epoch 70, Average Training Loss: 0.3688, Validation Loss: 0.3604\n",
      "Epoch 71, Average Training Loss: 0.3777, Validation Loss: 0.3621\n",
      "Epoch 72, Average Training Loss: 0.3858, Validation Loss: 0.3586\n",
      "Epoch 73, Average Training Loss: 0.3981, Validation Loss: 0.3643\n",
      "Epoch 74, Average Training Loss: 0.4093, Validation Loss: 0.3589\n",
      "Epoch 75, Average Training Loss: 0.3811, Validation Loss: 0.3588\n",
      "Epoch 76, Average Training Loss: 0.3779, Validation Loss: 0.3631\n",
      "Epoch 77, Average Training Loss: 0.3707, Validation Loss: 0.3620\n",
      "Epoch 78, Average Training Loss: 0.3810, Validation Loss: 0.3590\n",
      "Epoch 79, Average Training Loss: 0.3786, Validation Loss: 0.3667\n",
      "Epoch 80, Average Training Loss: 0.3694, Validation Loss: 0.3629\n"
     ]
    }
   ],
   "source": [
    "for (train_index, val_index) in kf.split(all_sentences):\n",
    "    print(f\"Fold {fold}\")\n",
    "    fold += 1\n",
    "    \n",
    "    train_sentences = [all_sentences[i] for i in train_index]\n",
    "    val_sentences = [all_sentences[i] for i in val_index]\n",
    "    train_tags = [all_tags[i] for i in train_index]\n",
    "    val_tags = [all_tags[i] for i in val_index]\n",
    "\n",
    "    train_dataset = LSTM_Attention_NERDataset(train_sentences, train_tags, word_encoder, tag_encoder, unknown_token)\n",
    "    val_dataset = LSTM_Attention_NERDataset(val_sentences, val_tags, word_encoder, tag_encoder, unknown_token)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=lambda x: x)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "    model = BiLSTM_CrossAttention_NER_Model(len(word_encoder), len(tag_encoder.classes_)).to(device)\n",
    "    criterion = Focal_Loss(alpha=1.3, gamma=0, ignore_index=-100) # See if the Focal Loss makes any difference\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.4)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    model.train()\n",
    "    loss_dic = {}\n",
    "    valid_loss_dic = {}\n",
    "\n",
    "    for epoch in range(80):\n",
    "        total_loss = 0\n",
    "        total_valid_loss = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            sentences, tags = zip(*batch)\n",
    "            sentences = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True).to(device)\n",
    "            tags = torch.nn.utils.rnn.pad_sequence(tags, batch_first=True, padding_value=-100).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(sentences)\n",
    "                outputs = outputs.view(-1, outputs.shape[-1])\n",
    "                tags = tags.view(-1)\n",
    "                loss = criterion(outputs, tags)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                val_sentences, val_tags = zip(*batch)\n",
    "                val_sentences = torch.nn.utils.rnn.pad_sequence(val_sentences, batch_first=True).to(device)\n",
    "                val_tags = torch.nn.utils.rnn.pad_sequence(val_tags, batch_first=True, padding_value=-100).to(device)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    val_outputs = model(val_sentences)\n",
    "                    val_outputs = val_outputs.view(-1, val_outputs.shape[-1])\n",
    "                    val_tags = val_tags.view(-1)\n",
    "                    valid_loss = criterion(val_outputs, val_tags)\n",
    "                    total_valid_loss += valid_loss.item()\n",
    "        \n",
    "        avg_valid_loss = total_valid_loss / len(val_dataloader)\n",
    "\n",
    "        scheduler.step(avg_train_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Average Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_valid_loss:.4f}\")\n",
    "        loss_dic[epoch + 1] = avg_train_loss\n",
    "        valid_loss_dic[epoch + 1] = avg_valid_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drawing the Training Loss vs Epochs\n",
    "graph_plot(\"Loss vs Epochs\", \"Epochs\", \"Loss\", list(loss_dic.keys()), list(loss_dic.values()), list(valid_loss_dic.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model, and evaluating the f1 score\n",
    "test_file = '../../Data/NCBItestset_corpus.txt'\n",
    "test_lines = read_dataset(test_file)\n",
    "test_paragraphs = parse_dataset(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing and storing the test dataset\n",
    "test_sentences = []\n",
    "test_tags = []\n",
    "\n",
    "for paragraph in test_paragraphs:\n",
    "    sentences, annotations = parse_paragraph(paragraph)\n",
    "    tagged_sentences = tag_annotations(sentences, annotations)\n",
    "    for sentence, tags in tagged_sentences:\n",
    "        test_sentences.append(sentence)\n",
    "        test_tags.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_CrossAttention_NER_Model(\n",
       "  (embedding): Embedding(14805, 128)\n",
       "  (batch_norm_emb): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bilstm): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.35, bidirectional=True)\n",
       "  (cross_attention): CrossAttention(\n",
       "    (query): Linear(in_features=256, out_features=256, bias=False)\n",
       "    (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "    (value): Linear(in_features=256, out_features=256, bias=False)\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (batch_norm_att): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.35, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the model file\n",
    "model = BiLSTM_CrossAttention_NER_Model(len(word_encoder), len(tag_encoder.classes_)).to(device)\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of <UNK> tokens in the testing dataset: 0\n",
      "Tags of <UNK> tokens in the testing dataset: Counter()\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "I-CompositeMention       0.61      0.25      0.35        89\n",
      "    I-DiseaseClass       0.39      0.20      0.26       255\n",
      "        I-Modifier       0.67      0.31      0.42       367\n",
      " I-SpecificDisease       0.49      0.57      0.53      1090\n",
      "                 O       0.96      0.97      0.97     18601\n",
      "\n",
      "          accuracy                           0.93     20402\n",
      "         macro avg       0.63      0.46      0.51     20402\n",
      "      weighted avg       0.92      0.93      0.92     20402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the test data\n",
    "test_dataset = LSTM_Attention_NERDataset(test_sentences, test_tags, word_encoder, tag_encoder, '<UNK>')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "# Display the counts and tags of <UNK> tokens\n",
    "print(f\"Number of <UNK> tokens in the testing dataset: {test_dataset.unk_count}\")\n",
    "print(f\"Tags of <UNK> tokens in the testing dataset: {Counter(test_dataset.unk_tags)}\")\n",
    "\n",
    "# Evaluate the model\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "result = \"../../Result/TestResults_CrossAttention_BiLSTM_NER.txt\"\n",
    "with open(result, 'w') as t_file:\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            sentences, tags = zip(*batch)\n",
    "            sentences = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True).to(device)\n",
    "            tags = torch.nn.utils.rnn.pad_sequence(tags, batch_first=True, padding_value=-100).to(device)\n",
    "\n",
    "            outputs = model(sentences)\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            true_labels = tags.cpu().numpy()\n",
    "\n",
    "            mask = true_labels != -100\n",
    "            pred_labels = predictions[mask]\n",
    "            true_labels = true_labels[mask]\n",
    "\n",
    "            pred_labels_decoded = tag_encoder.inverse_transform(pred_labels)\n",
    "            true_labels_decoded = tag_encoder.inverse_transform(true_labels)\n",
    "\n",
    "            for true_label, pred_label in zip(true_labels_decoded, pred_labels_decoded):\n",
    "                t_file.write(f'True: {true_label}, Pred: {pred_label}\\n')\n",
    "                all_true_labels.append(true_label)\n",
    "                all_pred_labels.append(pred_label)\n",
    "\n",
    "# Printing classification report\n",
    "report = classification_report(all_true_labels, all_pred_labels)\n",
    "print (report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61111111 0.39370079 0.6746988  0.48982786 0.96100027]\n",
      "[0.24719101 0.19607843 0.30517711 0.57431193 0.97102306]\n",
      "0.5233324016744941\n",
      "[0.3886658893356653, 0.2778421005244493, 0.45376494971729275, 0.5303904030983063, 0.9659986656967566]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Testing the G_Mean based on the heatmap of Confusion Matrix\n",
    "'''\n",
    "precision = precision_score(all_true_labels, all_pred_labels, average = None)\n",
    "recall = recall_score(all_true_labels, all_pred_labels, average = None)\n",
    "print(precision)\n",
    "print(recall)\n",
    "g_mean = []\n",
    "G_Mean = 0\n",
    "for p, r in zip(precision, recall):\n",
    "    class_gmean = sqrt(p*r)\n",
    "    g_mean.append(class_gmean)\n",
    "    G_Mean += class_gmean\n",
    "\n",
    "G_Mean /= len(precision)\n",
    "print (G_Mean)\n",
    "print(g_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a heatmap\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.heatmap(g_mean, annot=True, cmap='coolwarm', square=True)\n",
    "# plt.xlabel('G Mean')\n",
    "# plt.ylabel('Class')\n",
    "# plt.title(\"G Mean w.r.t Classes\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
