{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Based Cross Attention BiLSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Required Files and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Importing the relevant files\n",
    "train_file = '../../Data/NCBItrainset_corpus.txt'\n",
    "dev_file = '../../Data/NCBIdevelopset_corpus.txt'\n",
    "test_file = '../../DataNCBItestset_corpus.txt'\n",
    "model_name = '../../Models/BiLSTM_CrossAttention_NER_model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset file\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "def parse_dataset(lines):\n",
    "    paragraphs = []\n",
    "    paragraph = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            paragraph.append(line)\n",
    "        else:\n",
    "            if paragraph:\n",
    "                paragraphs.append(paragraph)\n",
    "                paragraph = []\n",
    "\n",
    "    if paragraph:\n",
    "        paragraphs.append(paragraph)\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "def parse_paragraph(paragraph):\n",
    "    sentences = []\n",
    "    annotations = []\n",
    "    sentence = []\n",
    "\n",
    "    for line in paragraph:\n",
    "        if re.match(r'^\\d+\\|\\w\\|', line):\n",
    "            sentence.extend(line.split('|')[2].split())\n",
    "\n",
    "        elif re.match(r'^\\d+\\t\\d+\\t\\d+\\t', line):\n",
    "            start, end = int(line.split(\"\\t\")[1]), int(line.split(\"\\t\")[2])\n",
    "            annotations.append((start, end, line.split(\"\\t\")[3], line.split(\"\\t\")[4]))\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Labelling\n",
    "def tag_annotations(sentences, annotations):\n",
    "    tagged_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tags = ['O'] * len(sentence)  # Set tags at \"O\"\n",
    "        word_starts = []\n",
    "        word_ends = []\n",
    "        char_pos = 0\n",
    "\n",
    "        for word in sentence:\n",
    "            word_starts.append(char_pos)\n",
    "            char_pos += len(word)\n",
    "            word_ends.append(char_pos)\n",
    "            char_pos += 1  # WhiteSpace Character\n",
    "\n",
    "        # Based on the character limits, change the annotations\n",
    "        # A custom IO tagging scheme is used\n",
    "        # Labels are assigned on the basis of disease label in annotations\n",
    "        for start, end, disease_info, label in annotations:\n",
    "            for i, (word_start, word_end) in enumerate(zip(word_starts, word_ends)):\n",
    "                if word_start >= start and word_end <= end:\n",
    "                    tags[i] = 'I-' + label\n",
    "                elif word_start < start < word_end or word_start < end < word_end:\n",
    "                    tags[i] = 'I-' + label\n",
    "\n",
    "        tagged_sentences.append((sentence, tags))\n",
    "\n",
    "    return tagged_sentences\n",
    "\n",
    "def format_for_model(tagged_data):\n",
    "    formatted_data = []\n",
    "    for words, tags in tagged_data:\n",
    "        sentence_data = '\\n'.join([f'{word}\\t{tag}' for word, tag in zip(words, tags)])\n",
    "        formatted_data.append(sentence_data)\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "lines = read_dataset(train_file)\n",
    "paragraphs = parse_dataset(lines)\n",
    "all_tagged_data = []\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    sentences, annotations = parse_paragraph(paragraph)\n",
    "    tagged_data = tag_annotations(sentences, annotations)\n",
    "    all_tagged_data.extend(tagged_data)\n",
    "\n",
    "formatted_data = format_for_model(all_tagged_data)\n",
    "\n",
    "Format_Data = \"../../Formatted Data/BiLSTM_CrossAttention_DataPrep.txt\"\n",
    "\n",
    "# Save formatted data to a file\n",
    "with open(Format_Data, 'w') as file:\n",
    "    file.write('\\n\\n'.join(formatted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "dev_lines = read_dataset(dev_file)\n",
    "dev_paragraphs = parse_dataset(dev_lines)\n",
    "dev_all_tagged_data = []\n",
    "\n",
    "for dev_paragraph in dev_paragraphs:\n",
    "    dev_sentences, dev_annotations = parse_paragraph(dev_paragraph)\n",
    "    dev_tagged_data = tag_annotations(dev_sentences, dev_annotations)\n",
    "    dev_all_tagged_data.extend(dev_tagged_data)\n",
    "\n",
    "dev_formatted_data = format_for_model(dev_all_tagged_data)\n",
    "\n",
    "Dev_Format_Data = \"../../Formatted Data/BiLSTM_CrossAttention_DevelopSet_DataPrep.txt\"\n",
    "\n",
    "# Save formatted data to a file\n",
    "with open(Dev_Format_Data, 'w') as file:\n",
    "    file.write('\\n\\n'.join(dev_formatted_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NER Dataset class\n",
    "class LSTM_NER_Dataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = self.load_data(file_path)\n",
    "        self.word_encoder = LabelEncoder()\n",
    "        self.tag_encoder = LabelEncoder()\n",
    "        self.sentences, self.tags = self.process_data()\n",
    "        self.word_encoder.fit([word for sent in self.sentences for word in sent])\n",
    "        self.tag_encoder.fit([tag for tag_seq in self.tags for tag in tag_seq])\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.read().split('\\n\\n')\n",
    "        return data\n",
    "\n",
    "    def process_data(self):\n",
    "        sentences = []\n",
    "        tags = []\n",
    "        for entry in self.data:\n",
    "            lines = entry.strip().split('\\n')\n",
    "            sentence = []\n",
    "            tag_seq = []\n",
    "            for line in lines:\n",
    "                word, tag = line.split('\\t')\n",
    "                sentence.append(word)\n",
    "                tag_seq.append(tag)\n",
    "            sentences.append(sentence)\n",
    "            tags.append(tag_seq)\n",
    "        return sentences, tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tags = self.tags[idx]\n",
    "        word_indices = self.word_encoder.transform(sentence)\n",
    "        tag_indices = self.tag_encoder.transform(tags)\n",
    "        return torch.tensor(word_indices, dtype=torch.long), torch.tensor(tag_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Cross Attention and Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Attention Class\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        query_proj = self.query(query)\n",
    "        key_proj = self.key(key)\n",
    "        value_proj = self.value(value)\n",
    "        attention_scores = torch.matmul(query_proj, key_proj.transpose(-2, -1))\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        context = torch.matmul(attention_weights, value_proj)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Class (Defining the Layers and the data flow through these layers)\n",
    "class BiLSTM_CrossAttention_NER_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=128, dropout_prob=0.3):\n",
    "        super(BiLSTM_CrossAttention_NER_Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.batch_norm_emb = nn.BatchNorm1d(embedding_dim)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=dropout_prob)\n",
    "        self.cross_attention = CrossAttention(hidden_dim * 2)\n",
    "        self.batch_norm_att = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = self.batch_norm_emb(emb.transpose(1, 2)).transpose(1, 2)\n",
    "        emb = self.dropout(emb)\n",
    "        \n",
    "        bilstm_out, _ = self.bilstm(emb)\n",
    "        bilstm_out = self.dropout(bilstm_out)\n",
    "        \n",
    "        cross_att_out = self.cross_attention(bilstm_out, bilstm_out, bilstm_out)\n",
    "        cross_att_out = self.batch_norm_att(cross_att_out.transpose(1, 2)).transpose(1, 2)\n",
    "        cross_att_out = self.dropout(cross_att_out)\n",
    "        \n",
    "        tag_space = self.fc(cross_att_out)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting (to be used to visualize the training loss and validation loss)\n",
    "# Used to figure if the mdoel is underfitting or overfitting\n",
    "def graph_plot(title, x_label, y_label, x_data, y_data, z_data, color = 'blue', linestyle = '-'):\n",
    "    plt.plot(x_data, y_data, color = color, linestyle = linestyle)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.plot(x_data, z_data, color = 'r', linestyle = '-')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"../../Graphs/aBiLSTM_CrossAttention.png\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
